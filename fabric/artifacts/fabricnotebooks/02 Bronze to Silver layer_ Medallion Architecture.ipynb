{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e108de0-da8d-4f16-bfe4-9c4d0cfa178c",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Transform data from Bronze 'Raw data' to Silver 'Cleansed and conformed data' layer and perform data transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7df7c0-4e24-45cc-ab93-4e55c1bdeb6d",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "![Medallion Architecture](https://fabricddib.blob.core.windows.net/notebookimage/MedallionArchitecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603729e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT * FROM #LAKEHOUSE_SILVER#.dimension_campaign LIMIT 1000\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e9203b-8af2-4870-86dd-4b1e3acec601",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Importing the necessary libraries and spark configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a5c0ecf-6c11-4caa-83cb-ffc12dde0fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "parent_msg_id": "17aabbf8-912d-4d1f-98e9-730ef76f276f",
       "queued_time": "2023-09-11T22:28:05.1991945Z",
       "session_id": null,
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.binSize\", \"1073741824\")\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, unix_timestamp, to_date,col,year,quarter,month\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import col, unix_timestamp, to_date,col,year,quarter,month\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c25d7f2-7474-4623-9266-f466f5d8f667",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Set input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e108c017-9863-48de-a37b-dc9a258ae7b1",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "parent_msg_id": "9d030883-35d0-46ce-9e2c-06e9c1f06a74",
       "queued_time": "2023-09-11T22:28:05.2076977Z",
       "session_id": null,
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Path_Dim='abfss://#SALES_WORKSPACE_NAME#@onelake.dfs.fabric.microsoft.com/#LAKEHOUSE_BRONZE#.Lakehouse/Files/DimensionData/'\n",
    "Path_Fact='abfss://#SALES_WORKSPACE_NAME#@onelake.dfs.fabric.microsoft.com/#LAKEHOUSE_BRONZE#.Lakehouse/Files/FactData/'\n",
    "Path_LitwareData='abfss://#SALES_WORKSPACE_NAME#@onelake.dfs.fabric.microsoft.com/#LAKEHOUSE_BRONZE#.Lakehouse/Files/sales-transaction-litware'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491bf3da-5ae1-4d35-999f-2e4f02c258ce",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Dimension - Brand\n",
    "We created a shortcut to raw data that we landed earlier in the Bronze lakehouse. We then do the necessary cleanup and transformation on the data and write the dimension table to the silver lakehouse in open standard delta parquet format. The table starts appearing under the tables pane as soon as we execute this cell. We follow the similar approach for rest of the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92d1f2c9-85f0-40bd-8c79-feabf86760d4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-09-11T18:46:30.7351905Z",
       "execution_start_time": "2023-09-11T18:46:13.9692377Z",
       "livy_statement_state": "available",
       "parent_msg_id": "b0b469e5-8af6-4efa-aed0-b95abce783cd",
       "queued_time": "2023-09-11T18:46:13.6394284Z",
       "session_id": "4259ac00-d937-4ed0-b39b-3a46ab7c1f3b",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [
         {
          "completionTime": "2023-09-11T18:46:29.135GMT",
          "dataRead": 4344,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 5:\ntable_name = 'dimension_brand'\ndimension_campaignchatgpt_schema = StructType([\n    StructField('BrandId', IntegerType(), True),\n    StructField('BrandName', StringType(), True),\n    StructField('EntityCode', StringType(), True)]\n)\ndf = spark.read.format(\"csv\").schema(dimension_campaignchatgpt_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 2",
          "displayName": "toString at String.java:2994",
          "jobGroup": "5",
          "jobId": 16,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 53,
          "numTasks": 54,
          "rowCount": 50,
          "stageIds": [
           27,
           28,
           26
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:29.056GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:29.032GMT",
          "dataRead": 4931,
          "dataWritten": 4344,
          "description": "Delta: Job group for statement 5:\ntable_name = 'dimension_brand'\ndimension_campaignchatgpt_schema = StructType([\n    StructField('BrandId', IntegerType(), True),\n    StructField('BrandName', StringType(), True),\n    StructField('EntityCode', StringType(), True)]\n)\ndf = spark.read.format(\"csv\").schema(dimension_campaignchatgpt_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 2",
          "displayName": "toString at String.java:2994",
          "jobGroup": "5",
          "jobId": 15,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 3,
          "numTasks": 53,
          "rowCount": 60,
          "stageIds": [
           24,
           25
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:28.257GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:28.064GMT",
          "dataRead": 3730,
          "dataWritten": 4931,
          "description": "Delta: Job group for statement 5:\ntable_name = 'dimension_brand'\ndimension_campaignchatgpt_schema = StructType([\n    StructField('BrandId', IntegerType(), True),\n    StructField('BrandName', StringType(), True),\n    StructField('EntityCode', StringType(), True)]\n)\ndf = spark.read.format(\"csv\").schema(dimension_campaignchatgpt_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 2",
          "displayName": "toString at String.java:2994",
          "jobGroup": "5",
          "jobId": 14,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 3,
          "numCompletedStages": 1,
          "numCompletedTasks": 3,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 3,
          "rowCount": 20,
          "stageIds": [
           23
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:27.936GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:27.346GMT",
          "dataRead": 1753,
          "dataWritten": 0,
          "description": "Job group for statement 5:\ntable_name = 'dimension_brand'\ndimension_campaignchatgpt_schema = StructType([\n    StructField('BrandId', IntegerType(), True),\n    StructField('BrandName', StringType(), True),\n    StructField('EntityCode', StringType(), True)]\n)\ndf = spark.read.format(\"csv\").schema(dimension_campaignchatgpt_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "5",
          "jobId": 13,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 2,
          "numTasks": 52,
          "rowCount": 3,
          "stageIds": [
           21,
           22
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:27.134GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:27.026GMT",
          "dataRead": 307,
          "dataWritten": 1817,
          "description": "Job group for statement 5:\ntable_name = 'dimension_brand'\ndimension_campaignchatgpt_schema = StructType([\n    StructField('BrandId', IntegerType(), True),\n    StructField('BrandName', StringType(), True),\n    StructField('EntityCode', StringType(), True)]\n)\ndf = spark.read.format(\"csv\").schema(dimension_campaignchatgpt_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "5",
          "jobId": 12,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 2,
          "rowCount": 14,
          "stageIds": [
           19,
           20
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:26.669GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:26.624GMT",
          "dataRead": 205,
          "dataWritten": 307,
          "description": "Job group for statement 5:\ntable_name = 'dimension_brand'\ndimension_campaignchatgpt_schema = StructType([\n    StructField('BrandId', IntegerType(), True),\n    StructField('BrandName', StringType(), True),\n    StructField('EntityCode', StringType(), True)]\n)\ndf = spark.read.format(\"csv\").schema(dimension_campaignchatgpt_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "5",
          "jobId": 11,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 14,
          "stageIds": [
           18
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:26.425GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:26.037GMT",
          "dataRead": 4339,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 5:\ntable_name = 'dimension_brand'\ndimension_campaignchatgpt_schema = StructType([\n    StructField('BrandId', IntegerType(), True),\n    StructField('BrandName', StringType(), True),\n    StructField('EntityCode', StringType(), True)]\n)\ndf = spark.read.format(\"csv\").schema(dimension_campaignchatgpt_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "5",
          "jobId": 10,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 52,
          "numTasks": 53,
          "rowCount": 50,
          "stageIds": [
           15,
           16,
           17
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:25.981GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:25.956GMT",
          "dataRead": 3229,
          "dataWritten": 4339,
          "description": "Delta: Job group for statement 5:\ntable_name = 'dimension_brand'\ndimension_campaignchatgpt_schema = StructType([\n    StructField('BrandId', IntegerType(), True),\n    StructField('BrandName', StringType(), True),\n    StructField('EntityCode', StringType(), True)]\n)\ndf = spark.read.format(\"csv\").schema(dimension_campaignchatgpt_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "5",
          "jobId": 9,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 2,
          "numTasks": 52,
          "rowCount": 57,
          "stageIds": [
           13,
           14
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:25.167GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:24.988GMT",
          "dataRead": 2556,
          "dataWritten": 3229,
          "description": "Delta: Job group for statement 5:\ntable_name = 'dimension_brand'\ndimension_campaignchatgpt_schema = StructType([\n    StructField('BrandId', IntegerType(), True),\n    StructField('BrandName', StringType(), True),\n    StructField('EntityCode', StringType(), True)]\n)\ndf = spark.read.format(\"csv\").schema(dimension_campaignchatgpt_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "5",
          "jobId": 8,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 2,
          "numCompletedStages": 1,
          "numCompletedTasks": 2,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 2,
          "rowCount": 14,
          "stageIds": [
           12
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:24.678GMT",
          "usageDescription": ""
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 9,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 5
      },
      "text/plain": [
       "StatementMeta(, 4259ac00-d937-4ed0-b39b-3a46ab7c1f3b, 5, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table_name = 'dimension_brand'\n",
    "dimension_campaignchatgpt_schema = StructType([\n",
    "    StructField('BrandId', IntegerType(), True),\n",
    "    StructField('BrandName', StringType(), True),\n",
    "    StructField('EntityCode', StringType(), True)]\n",
    ")\n",
    "df = spark.read.format(\"csv\").schema(dimension_campaignchatgpt_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n",
    "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\"+table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c522ba39-1f0b-4ba7-b3d0-6f5f5f2303fb",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Dimension-Campaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfb88cdb-57ed-4be6-89fb-332cff83c369",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-09-11T18:46:36.0877088Z",
       "execution_start_time": "2023-09-11T18:46:31.1519691Z",
       "livy_statement_state": "available",
       "parent_msg_id": "669d53dd-6fca-4a41-a948-099ca761f789",
       "queued_time": "2023-09-11T18:46:13.9189799Z",
       "session_id": "4259ac00-d937-4ed0-b39b-3a46ab7c1f3b",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [
         {
          "completionTime": "2023-09-11T18:46:35.166GMT",
          "dataRead": 4344,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 6:\ntable_name = 'dimension_campaign'\ndimension_campaign_schema = StructType([\n    StructField('Campaigns_ID', IntegerType(), True), \n    StructField('CampaignName', StringType(), True), \n    StructField('SubCampaignId', StringType(), True)] \n)\ndf = spark.read.format(\"csv\").schema(dimension_campaign_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf = df.withColumnRenamed(\"Campaigns_ID\", \"CampaignId\")\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 2",
          "displayName": "toString at String.java:2994",
          "jobGroup": "6",
          "jobId": 25,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 53,
          "numTasks": 54,
          "rowCount": 50,
          "stageIds": [
           45,
           43,
           44
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:35.100GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:35.072GMT",
          "dataRead": 4834,
          "dataWritten": 4344,
          "description": "Delta: Job group for statement 6:\ntable_name = 'dimension_campaign'\ndimension_campaign_schema = StructType([\n    StructField('Campaigns_ID', IntegerType(), True), \n    StructField('CampaignName', StringType(), True), \n    StructField('SubCampaignId', StringType(), True)] \n)\ndf = spark.read.format(\"csv\").schema(dimension_campaign_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf = df.withColumnRenamed(\"Campaigns_ID\", \"CampaignId\")\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 2",
          "displayName": "toString at String.java:2994",
          "jobGroup": "6",
          "jobId": 24,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 3,
          "numTasks": 53,
          "rowCount": 60,
          "stageIds": [
           42,
           41
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:34.420GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:34.305GMT",
          "dataRead": 3788,
          "dataWritten": 4834,
          "description": "Delta: Job group for statement 6:\ntable_name = 'dimension_campaign'\ndimension_campaign_schema = StructType([\n    StructField('Campaigns_ID', IntegerType(), True), \n    StructField('CampaignName', StringType(), True), \n    StructField('SubCampaignId', StringType(), True)] \n)\ndf = spark.read.format(\"csv\").schema(dimension_campaign_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf = df.withColumnRenamed(\"Campaigns_ID\", \"CampaignId\")\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 2",
          "displayName": "toString at String.java:2994",
          "jobGroup": "6",
          "jobId": 23,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 3,
          "numCompletedStages": 1,
          "numCompletedTasks": 3,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 3,
          "rowCount": 20,
          "stageIds": [
           40
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:34.225GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:33.739GMT",
          "dataRead": 1743,
          "dataWritten": 0,
          "description": "Job group for statement 6:\ntable_name = 'dimension_campaign'\ndimension_campaign_schema = StructType([\n    StructField('Campaigns_ID', IntegerType(), True), \n    StructField('CampaignName', StringType(), True), \n    StructField('SubCampaignId', StringType(), True)] \n)\ndf = spark.read.format(\"csv\").schema(dimension_campaign_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf = df.withColumnRenamed(\"Campaigns_ID\", \"CampaignId\")\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "6",
          "jobId": 22,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 2,
          "numTasks": 52,
          "rowCount": 3,
          "stageIds": [
           38,
           39
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:33.577GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:33.431GMT",
          "dataRead": 298,
          "dataWritten": 1988,
          "description": "Job group for statement 6:\ntable_name = 'dimension_campaign'\ndimension_campaign_schema = StructType([\n    StructField('Campaigns_ID', IntegerType(), True), \n    StructField('CampaignName', StringType(), True), \n    StructField('SubCampaignId', StringType(), True)] \n)\ndf = spark.read.format(\"csv\").schema(dimension_campaign_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf = df.withColumnRenamed(\"Campaigns_ID\", \"CampaignId\")\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "6",
          "jobId": 21,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 2,
          "rowCount": 14,
          "stageIds": [
           37,
           36
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:33.116GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:33.078GMT",
          "dataRead": 221,
          "dataWritten": 298,
          "description": "Job group for statement 6:\ntable_name = 'dimension_campaign'\ndimension_campaign_schema = StructType([\n    StructField('Campaigns_ID', IntegerType(), True), \n    StructField('CampaignName', StringType(), True), \n    StructField('SubCampaignId', StringType(), True)] \n)\ndf = spark.read.format(\"csv\").schema(dimension_campaign_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf = df.withColumnRenamed(\"Campaigns_ID\", \"CampaignId\")\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "6",
          "jobId": 20,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 14,
          "stageIds": [
           35
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:33.000GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:32.761GMT",
          "dataRead": 4339,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 6:\ntable_name = 'dimension_campaign'\ndimension_campaign_schema = StructType([\n    StructField('Campaigns_ID', IntegerType(), True), \n    StructField('CampaignName', StringType(), True), \n    StructField('SubCampaignId', StringType(), True)] \n)\ndf = spark.read.format(\"csv\").schema(dimension_campaign_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf = df.withColumnRenamed(\"Campaigns_ID\", \"CampaignId\")\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "6",
          "jobId": 19,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 52,
          "numTasks": 53,
          "rowCount": 50,
          "stageIds": [
           33,
           34,
           32
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:32.708GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:32.688GMT",
          "dataRead": 3130,
          "dataWritten": 4339,
          "description": "Delta: Job group for statement 6:\ntable_name = 'dimension_campaign'\ndimension_campaign_schema = StructType([\n    StructField('Campaigns_ID', IntegerType(), True), \n    StructField('CampaignName', StringType(), True), \n    StructField('SubCampaignId', StringType(), True)] \n)\ndf = spark.read.format(\"csv\").schema(dimension_campaign_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf = df.withColumnRenamed(\"Campaigns_ID\", \"CampaignId\")\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "6",
          "jobId": 18,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 2,
          "numTasks": 52,
          "rowCount": 57,
          "stageIds": [
           30,
           31
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:32.031GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:31.821GMT",
          "dataRead": 2576,
          "dataWritten": 3130,
          "description": "Delta: Job group for statement 6:\ntable_name = 'dimension_campaign'\ndimension_campaign_schema = StructType([\n    StructField('Campaigns_ID', IntegerType(), True), \n    StructField('CampaignName', StringType(), True), \n    StructField('SubCampaignId', StringType(), True)] \n)\ndf = spark.read.format(\"csv\").schema(dimension_campaign_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf = df.withColumnRenamed(\"Campaigns_ID\", \"CampaignId\")\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "6",
          "jobId": 17,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 2,
          "numCompletedStages": 1,
          "numCompletedTasks": 2,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 2,
          "rowCount": 14,
          "stageIds": [
           29
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:31.716GMT",
          "usageDescription": ""
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 9,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 6
      },
      "text/plain": [
       "StatementMeta(, 4259ac00-d937-4ed0-b39b-3a46ab7c1f3b, 6, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table_name = 'dimension_campaign'\n",
    "dimension_campaign_schema = StructType([\n",
    "    StructField('Campaigns_ID', IntegerType(), True), \n",
    "    StructField('CampaignName', StringType(), True), \n",
    "    StructField('SubCampaignId', StringType(), True)] \n",
    ")\n",
    "df = spark.read.format(\"csv\").schema(dimension_campaign_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n",
    "df = df.withColumnRenamed(\"Campaigns_ID\", \"CampaignId\")\n",
    "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\" + table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c63b1b-13dd-4e13-b163-02c085780347",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Dimension-Customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bb88200-7051-40ed-8ba5-a1357b36e0a2",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-09-11T18:46:41.3722353Z",
       "execution_start_time": "2023-09-11T18:46:36.4959518Z",
       "livy_statement_state": "available",
       "parent_msg_id": "3a90aadd-e10f-4d51-b0db-cfe3ef4f99c0",
       "queued_time": "2023-09-11T18:46:14.3761114Z",
       "session_id": "4259ac00-d937-4ed0-b39b-3a46ab7c1f3b",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [
         {
          "completionTime": "2023-09-11T18:46:40.408GMT",
          "dataRead": 4426,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 7:\ntable_name = 'dimension_customer'\ndimension_customer_schema = StructType([\n    StructField('Id', IntegerType(), True), \n    StructField('Age', IntegerType(), True), \n    StructField('Gender', StringType(), True), \n    StructField('Pincode', StringType(), True), \n    StructField('FirstName', StringType(), True), \n    StructField('LastName', StringType(), True), \n    StructField('FullName', StringType(), True), \n    StructField('DateOfBirth', StringType(), True), \n    StructField('Address', StringType(), True), \n    StructField('Email', StringType(), True), \n    StructField('Mobile', StringType(), True),\n    StructField('UserName', StringType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_customer_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 3",
          "displayName": "toString at String.java:2994",
          "jobGroup": "7",
          "jobId": 34,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 54,
          "numTasks": 55,
          "rowCount": 50,
          "stageIds": [
           60,
           61,
           62
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:40.364GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:40.348GMT",
          "dataRead": 6710,
          "dataWritten": 4426,
          "description": "Delta: Job group for statement 7:\ntable_name = 'dimension_customer'\ndimension_customer_schema = StructType([\n    StructField('Id', IntegerType(), True), \n    StructField('Age', IntegerType(), True), \n    StructField('Gender', StringType(), True), \n    StructField('Pincode', StringType(), True), \n    StructField('FirstName', StringType(), True), \n    StructField('LastName', StringType(), True), \n    StructField('FullName', StringType(), True), \n    StructField('DateOfBirth', StringType(), True), \n    StructField('Address', StringType(), True), \n    StructField('Email', StringType(), True), \n    StructField('Mobile', StringType(), True),\n    StructField('UserName', StringType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_customer_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 3",
          "displayName": "toString at String.java:2994",
          "jobGroup": "7",
          "jobId": 33,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 4,
          "numTasks": 54,
          "rowCount": 61,
          "stageIds": [
           58,
           59
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:39.783GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:39.660GMT",
          "dataRead": 6649,
          "dataWritten": 6710,
          "description": "Delta: Job group for statement 7:\ntable_name = 'dimension_customer'\ndimension_customer_schema = StructType([\n    StructField('Id', IntegerType(), True), \n    StructField('Age', IntegerType(), True), \n    StructField('Gender', StringType(), True), \n    StructField('Pincode', StringType(), True), \n    StructField('FirstName', StringType(), True), \n    StructField('LastName', StringType(), True), \n    StructField('FullName', StringType(), True), \n    StructField('DateOfBirth', StringType(), True), \n    StructField('Address', StringType(), True), \n    StructField('Email', StringType(), True), \n    StructField('Mobile', StringType(), True),\n    StructField('UserName', StringType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_customer_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 3",
          "displayName": "toString at String.java:2994",
          "jobGroup": "7",
          "jobId": 32,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 4,
          "numCompletedStages": 1,
          "numCompletedTasks": 4,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 4,
          "rowCount": 22,
          "stageIds": [
           57
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:39.569GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:39.126GMT",
          "dataRead": 2187,
          "dataWritten": 0,
          "description": "Job group for statement 7:\ntable_name = 'dimension_customer'\ndimension_customer_schema = StructType([\n    StructField('Id', IntegerType(), True), \n    StructField('Age', IntegerType(), True), \n    StructField('Gender', StringType(), True), \n    StructField('Pincode', StringType(), True), \n    StructField('FirstName', StringType(), True), \n    StructField('LastName', StringType(), True), \n    StructField('FullName', StringType(), True), \n    StructField('DateOfBirth', StringType(), True), \n    StructField('Address', StringType(), True), \n    StructField('Email', StringType(), True), \n    StructField('Mobile', StringType(), True),\n    StructField('UserName', StringType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_customer_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "7",
          "jobId": 31,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 3,
          "numTasks": 53,
          "rowCount": 3,
          "stageIds": [
           56,
           55
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:38.900GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:38.812GMT",
          "dataRead": 763267,
          "dataWritten": 572032,
          "description": "Job group for statement 7:\ntable_name = 'dimension_customer'\ndimension_customer_schema = StructType([\n    StructField('Id', IntegerType(), True), \n    StructField('Age', IntegerType(), True), \n    StructField('Gender', StringType(), True), \n    StructField('Pincode', StringType(), True), \n    StructField('FirstName', StringType(), True), \n    StructField('LastName', StringType(), True), \n    StructField('FullName', StringType(), True), \n    StructField('DateOfBirth', StringType(), True), \n    StructField('Address', StringType(), True), \n    StructField('Email', StringType(), True), \n    StructField('Mobile', StringType(), True),\n    StructField('UserName', StringType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_customer_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "7",
          "jobId": 30,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 2,
          "rowCount": 10000,
          "stageIds": [
           53,
           54
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:38.381GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:38.331GMT",
          "dataRead": 820388,
          "dataWritten": 763267,
          "description": "Job group for statement 7:\ntable_name = 'dimension_customer'\ndimension_customer_schema = StructType([\n    StructField('Id', IntegerType(), True), \n    StructField('Age', IntegerType(), True), \n    StructField('Gender', StringType(), True), \n    StructField('Pincode', StringType(), True), \n    StructField('FirstName', StringType(), True), \n    StructField('LastName', StringType(), True), \n    StructField('FullName', StringType(), True), \n    StructField('DateOfBirth', StringType(), True), \n    StructField('Address', StringType(), True), \n    StructField('Email', StringType(), True), \n    StructField('Mobile', StringType(), True),\n    StructField('UserName', StringType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_customer_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "7",
          "jobId": 29,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 10000,
          "stageIds": [
           52
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:38.081GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:37.837GMT",
          "dataRead": 4421,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 7:\ntable_name = 'dimension_customer'\ndimension_customer_schema = StructType([\n    StructField('Id', IntegerType(), True), \n    StructField('Age', IntegerType(), True), \n    StructField('Gender', StringType(), True), \n    StructField('Pincode', StringType(), True), \n    StructField('FirstName', StringType(), True), \n    StructField('LastName', StringType(), True), \n    StructField('FullName', StringType(), True), \n    StructField('DateOfBirth', StringType(), True), \n    StructField('Address', StringType(), True), \n    StructField('Email', StringType(), True), \n    StructField('Mobile', StringType(), True),\n    StructField('UserName', StringType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_customer_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 2",
          "displayName": "toString at String.java:2994",
          "jobGroup": "7",
          "jobId": 28,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 53,
          "numTasks": 54,
          "rowCount": 50,
          "stageIds": [
           51,
           49,
           50
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:37.801GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:37.780GMT",
          "dataRead": 4646,
          "dataWritten": 4421,
          "description": "Delta: Job group for statement 7:\ntable_name = 'dimension_customer'\ndimension_customer_schema = StructType([\n    StructField('Id', IntegerType(), True), \n    StructField('Age', IntegerType(), True), \n    StructField('Gender', StringType(), True), \n    StructField('Pincode', StringType(), True), \n    StructField('FirstName', StringType(), True), \n    StructField('LastName', StringType(), True), \n    StructField('FullName', StringType(), True), \n    StructField('DateOfBirth', StringType(), True), \n    StructField('Address', StringType(), True), \n    StructField('Email', StringType(), True), \n    StructField('Mobile', StringType(), True),\n    StructField('UserName', StringType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_customer_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 2",
          "displayName": "toString at String.java:2994",
          "jobGroup": "7",
          "jobId": 27,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 3,
          "numTasks": 53,
          "rowCount": 58,
          "stageIds": [
           48,
           47
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:37.259GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:37.115GMT",
          "dataRead": 4859,
          "dataWritten": 4646,
          "description": "Delta: Job group for statement 7:\ntable_name = 'dimension_customer'\ndimension_customer_schema = StructType([\n    StructField('Id', IntegerType(), True), \n    StructField('Age', IntegerType(), True), \n    StructField('Gender', StringType(), True), \n    StructField('Pincode', StringType(), True), \n    StructField('FirstName', StringType(), True), \n    StructField('LastName', StringType(), True), \n    StructField('FullName', StringType(), True), \n    StructField('DateOfBirth', StringType(), True), \n    StructField('Address', StringType(), True), \n    StructField('Email', StringType(), True), \n    StructField('Mobile', StringType(), True),\n    StructField('UserName', StringType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_customer_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 2",
          "displayName": "toString at String.java:2994",
          "jobGroup": "7",
          "jobId": 26,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 3,
          "numCompletedStages": 1,
          "numCompletedTasks": 3,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 3,
          "rowCount": 16,
          "stageIds": [
           46
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:37.037GMT",
          "usageDescription": ""
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 9,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 7
      },
      "text/plain": [
       "StatementMeta(, 4259ac00-d937-4ed0-b39b-3a46ab7c1f3b, 7, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table_name = 'dimension_customer'\n",
    "dimension_customer_schema = StructType([\n",
    "    StructField('Id', IntegerType(), True), \n",
    "    StructField('Age', IntegerType(), True), \n",
    "    StructField('Gender', StringType(), True), \n",
    "    StructField('Pincode', StringType(), True), \n",
    "    StructField('FirstName', StringType(), True), \n",
    "    StructField('LastName', StringType(), True), \n",
    "    StructField('FullName', StringType(), True), \n",
    "    StructField('DateOfBirth', StringType(), True), \n",
    "    StructField('Address', StringType(), True), \n",
    "    StructField('Email', StringType(), True), \n",
    "    StructField('Mobile', StringType(), True),\n",
    "    StructField('UserName', StringType(), True)])\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(dimension_customer_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n",
    "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\" + table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb5f422-22e2-427e-8974-19af5aba7f90",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Dimension-Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e89e399-8525-420f-9079-278cbdeae04e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-09-11T18:46:46.6476207Z",
       "execution_start_time": "2023-09-11T18:46:41.7709557Z",
       "livy_statement_state": "available",
       "parent_msg_id": "e1f2e0ae-b051-4a71-84c3-12fe9d3e1a5d",
       "queued_time": "2023-09-11T18:46:15.0777909Z",
       "session_id": "4259ac00-d937-4ed0-b39b-3a46ab7c1f3b",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [
         {
          "completionTime": "2023-09-11T18:46:45.178GMT",
          "dataRead": 4425,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 8:\ntable_name = 'dimension_date'\ndimension_date_schema = StructType([\n    StructField('DateKey', IntegerType(), True), \n    StructField('DateValue', TimestampType(), True), \n    StructField('DayOfMonth', IntegerType(), True), \n    StructField('DayOfYear', IntegerType(), True), \n    StructField('Year', IntegerType(), True), \n    StructField('MonthOfYear', IntegerType(), True), \n    StructField('MonthName', StringType(), True), \n    StructField('QuarterOfYear', IntegerType(), True), \n    StructField('QuarterName', StringType(), True), \n    StructField('WeekEnding', TimestampType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_date_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 2",
          "displayName": "toString at String.java:2994",
          "jobGroup": "8",
          "jobId": 43,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 53,
          "numTasks": 54,
          "rowCount": 50,
          "stageIds": [
           78,
           79,
           77
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:45.146GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:45.132GMT",
          "dataRead": 4359,
          "dataWritten": 4425,
          "description": "Delta: Job group for statement 8:\ntable_name = 'dimension_date'\ndimension_date_schema = StructType([\n    StructField('DateKey', IntegerType(), True), \n    StructField('DateValue', TimestampType(), True), \n    StructField('DayOfMonth', IntegerType(), True), \n    StructField('DayOfYear', IntegerType(), True), \n    StructField('Year', IntegerType(), True), \n    StructField('MonthOfYear', IntegerType(), True), \n    StructField('MonthName', StringType(), True), \n    StructField('QuarterOfYear', IntegerType(), True), \n    StructField('QuarterName', StringType(), True), \n    StructField('WeekEnding', TimestampType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_date_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 2",
          "displayName": "toString at String.java:2994",
          "jobGroup": "8",
          "jobId": 42,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 3,
          "numTasks": 53,
          "rowCount": 58,
          "stageIds": [
           75,
           76
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:44.646GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:44.539GMT",
          "dataRead": 4509,
          "dataWritten": 4359,
          "description": "Delta: Job group for statement 8:\ntable_name = 'dimension_date'\ndimension_date_schema = StructType([\n    StructField('DateKey', IntegerType(), True), \n    StructField('DateValue', TimestampType(), True), \n    StructField('DayOfMonth', IntegerType(), True), \n    StructField('DayOfYear', IntegerType(), True), \n    StructField('Year', IntegerType(), True), \n    StructField('MonthOfYear', IntegerType(), True), \n    StructField('MonthName', StringType(), True), \n    StructField('QuarterOfYear', IntegerType(), True), \n    StructField('QuarterName', StringType(), True), \n    StructField('WeekEnding', TimestampType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_date_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 2",
          "displayName": "toString at String.java:2994",
          "jobGroup": "8",
          "jobId": 41,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 3,
          "numCompletedStages": 1,
          "numCompletedTasks": 3,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 3,
          "rowCount": 16,
          "stageIds": [
           74
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:44.478GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:44.030GMT",
          "dataRead": 1476,
          "dataWritten": 0,
          "description": "Job group for statement 8:\ntable_name = 'dimension_date'\ndimension_date_schema = StructType([\n    StructField('DateKey', IntegerType(), True), \n    StructField('DateValue', TimestampType(), True), \n    StructField('DayOfMonth', IntegerType(), True), \n    StructField('DayOfYear', IntegerType(), True), \n    StructField('Year', IntegerType(), True), \n    StructField('MonthOfYear', IntegerType(), True), \n    StructField('MonthName', StringType(), True), \n    StructField('QuarterOfYear', IntegerType(), True), \n    StructField('QuarterName', StringType(), True), \n    StructField('WeekEnding', TimestampType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_date_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "8",
          "jobId": 40,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 2,
          "numTasks": 52,
          "rowCount": 2,
          "stageIds": [
           72,
           73
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:43.891GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:43.804GMT",
          "dataRead": 31557,
          "dataWritten": 22160,
          "description": "Job group for statement 8:\ntable_name = 'dimension_date'\ndimension_date_schema = StructType([\n    StructField('DateKey', IntegerType(), True), \n    StructField('DateValue', TimestampType(), True), \n    StructField('DayOfMonth', IntegerType(), True), \n    StructField('DayOfYear', IntegerType(), True), \n    StructField('Year', IntegerType(), True), \n    StructField('MonthOfYear', IntegerType(), True), \n    StructField('MonthName', StringType(), True), \n    StructField('QuarterOfYear', IntegerType(), True), \n    StructField('QuarterName', StringType(), True), \n    StructField('WeekEnding', TimestampType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_date_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "8",
          "jobId": 39,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 2,
          "rowCount": 2190,
          "stageIds": [
           70,
           71
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:43.466GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:43.423GMT",
          "dataRead": 99538,
          "dataWritten": 31557,
          "description": "Job group for statement 8:\ntable_name = 'dimension_date'\ndimension_date_schema = StructType([\n    StructField('DateKey', IntegerType(), True), \n    StructField('DateValue', TimestampType(), True), \n    StructField('DayOfMonth', IntegerType(), True), \n    StructField('DayOfYear', IntegerType(), True), \n    StructField('Year', IntegerType(), True), \n    StructField('MonthOfYear', IntegerType(), True), \n    StructField('MonthName', StringType(), True), \n    StructField('QuarterOfYear', IntegerType(), True), \n    StructField('QuarterName', StringType(), True), \n    StructField('WeekEnding', TimestampType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_date_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "8",
          "jobId": 38,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 2190,
          "stageIds": [
           69
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:43.302GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:43.067GMT",
          "dataRead": 4412,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 8:\ntable_name = 'dimension_date'\ndimension_date_schema = StructType([\n    StructField('DateKey', IntegerType(), True), \n    StructField('DateValue', TimestampType(), True), \n    StructField('DayOfMonth', IntegerType(), True), \n    StructField('DayOfYear', IntegerType(), True), \n    StructField('Year', IntegerType(), True), \n    StructField('MonthOfYear', IntegerType(), True), \n    StructField('MonthName', StringType(), True), \n    StructField('QuarterOfYear', IntegerType(), True), \n    StructField('QuarterName', StringType(), True), \n    StructField('WeekEnding', TimestampType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_date_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "8",
          "jobId": 37,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 52,
          "numTasks": 53,
          "rowCount": 50,
          "stageIds": [
           66,
           67,
           68
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:43.040GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:43.023GMT",
          "dataRead": 2878,
          "dataWritten": 4412,
          "description": "Delta: Job group for statement 8:\ntable_name = 'dimension_date'\ndimension_date_schema = StructType([\n    StructField('DateKey', IntegerType(), True), \n    StructField('DateValue', TimestampType(), True), \n    StructField('DayOfMonth', IntegerType(), True), \n    StructField('DayOfYear', IntegerType(), True), \n    StructField('Year', IntegerType(), True), \n    StructField('MonthOfYear', IntegerType(), True), \n    StructField('MonthName', StringType(), True), \n    StructField('QuarterOfYear', IntegerType(), True), \n    StructField('QuarterName', StringType(), True), \n    StructField('WeekEnding', TimestampType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_date_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "8",
          "jobId": 36,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 2,
          "numTasks": 52,
          "rowCount": 56,
          "stageIds": [
           64,
           65
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:42.519GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:42.319GMT",
          "dataRead": 3098,
          "dataWritten": 2878,
          "description": "Delta: Job group for statement 8:\ntable_name = 'dimension_date'\ndimension_date_schema = StructType([\n    StructField('DateKey', IntegerType(), True), \n    StructField('DateValue', TimestampType(), True), \n    StructField('DayOfMonth', IntegerType(), True), \n    StructField('DayOfYear', IntegerType(), True), \n    StructField('Year', IntegerType(), True), \n    StructField('MonthOfYear', IntegerType(), True), \n    StructField('MonthName', StringType(), True), \n    StructField('QuarterOfYear', IntegerType(), True), \n    StructField('QuarterName', StringType(), True), \n    StructField('WeekEnding', TimestampType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_date_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "8",
          "jobId": 35,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 2,
          "numCompletedStages": 1,
          "numCompletedTasks": 2,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 2,
          "rowCount": 12,
          "stageIds": [
           63
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:42.267GMT",
          "usageDescription": ""
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 9,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 8
      },
      "text/plain": [
       "StatementMeta(, 4259ac00-d937-4ed0-b39b-3a46ab7c1f3b, 8, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table_name = 'dimension_date'\n",
    "dimension_date_schema = StructType([\n",
    "    StructField('DateKey', IntegerType(), True), \n",
    "    StructField('DateValue', TimestampType(), True), \n",
    "    StructField('DayOfMonth', IntegerType(), True), \n",
    "    StructField('DayOfYear', IntegerType(), True), \n",
    "    StructField('Year', IntegerType(), True), \n",
    "    StructField('MonthOfYear', IntegerType(), True), \n",
    "    StructField('MonthName', StringType(), True), \n",
    "    StructField('QuarterOfYear', IntegerType(), True), \n",
    "    StructField('QuarterName', StringType(), True), \n",
    "    StructField('WeekEnding', TimestampType(), True)])\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(dimension_date_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n",
    "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\" + table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef34297-a7f7-411f-8eb6-9c944144a489",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Dimension-Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1842727e-aaa0-4956-a4f3-692005c65624",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-09-11T18:46:52.0465571Z",
       "execution_start_time": "2023-09-11T18:46:47.0884743Z",
       "livy_statement_state": "available",
       "parent_msg_id": "a3e4505e-1a69-4aac-bf6b-8d5917364703",
       "queued_time": "2023-09-11T18:46:15.8053987Z",
       "session_id": "4259ac00-d937-4ed0-b39b-3a46ab7c1f3b",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [
         {
          "completionTime": "2023-09-11T18:46:50.159GMT",
          "dataRead": 4336,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 9:\ntable_name = 'dimension_country'\ndimension_country_schema = StructType([\n    StructField('ID', IntegerType(), True),\n    StructField('Country', StringType(), True), \n    StructField('Region', StringType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_country_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 3",
          "displayName": "toString at String.java:2994",
          "jobGroup": "9",
          "jobId": 52,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 54,
          "numTasks": 55,
          "rowCount": 50,
          "stageIds": [
           96,
           94,
           95
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:50.132GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:50.116GMT",
          "dataRead": 6460,
          "dataWritten": 4336,
          "description": "Delta: Job group for statement 9:\ntable_name = 'dimension_country'\ndimension_country_schema = StructType([\n    StructField('ID', IntegerType(), True),\n    StructField('Country', StringType(), True), \n    StructField('Region', StringType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_country_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 3",
          "displayName": "toString at String.java:2994",
          "jobGroup": "9",
          "jobId": 51,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 4,
          "numTasks": 54,
          "rowCount": 63,
          "stageIds": [
           93,
           92
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:49.683GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:49.479GMT",
          "dataRead": 4691,
          "dataWritten": 6460,
          "description": "Delta: Job group for statement 9:\ntable_name = 'dimension_country'\ndimension_country_schema = StructType([\n    StructField('ID', IntegerType(), True),\n    StructField('Country', StringType(), True), \n    StructField('Region', StringType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_country_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 3",
          "displayName": "toString at String.java:2994",
          "jobGroup": "9",
          "jobId": 50,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 4,
          "numCompletedStages": 1,
          "numCompletedTasks": 4,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 4,
          "rowCount": 26,
          "stageIds": [
           91
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:49.415GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:48.997GMT",
          "dataRead": 1732,
          "dataWritten": 0,
          "description": "Job group for statement 9:\ntable_name = 'dimension_country'\ndimension_country_schema = StructType([\n    StructField('ID', IntegerType(), True),\n    StructField('Country', StringType(), True), \n    StructField('Region', StringType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_country_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "9",
          "jobId": 49,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 3,
          "numTasks": 53,
          "rowCount": 3,
          "stageIds": [
           89,
           90
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:48.879GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:48.804GMT",
          "dataRead": 634,
          "dataWritten": 1960,
          "description": "Job group for statement 9:\ntable_name = 'dimension_country'\ndimension_country_schema = StructType([\n    StructField('ID', IntegerType(), True),\n    StructField('Country', StringType(), True), \n    StructField('Region', StringType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_country_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "9",
          "jobId": 48,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 2,
          "rowCount": 68,
          "stageIds": [
           88,
           87
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:48.574GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:48.536GMT",
          "dataRead": 704,
          "dataWritten": 634,
          "description": "Job group for statement 9:\ntable_name = 'dimension_country'\ndimension_country_schema = StructType([\n    StructField('ID', IntegerType(), True),\n    StructField('Country', StringType(), True), \n    StructField('Region', StringType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_country_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "9",
          "jobId": 47,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 68,
          "stageIds": [
           86
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:48.479GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:48.272GMT",
          "dataRead": 4331,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 9:\ntable_name = 'dimension_country'\ndimension_country_schema = StructType([\n    StructField('ID', IntegerType(), True),\n    StructField('Country', StringType(), True), \n    StructField('Region', StringType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_country_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 2",
          "displayName": "toString at String.java:2994",
          "jobGroup": "9",
          "jobId": 46,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 53,
          "numTasks": 54,
          "rowCount": 50,
          "stageIds": [
           84,
           85,
           83
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:48.246GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:48.221GMT",
          "dataRead": 4772,
          "dataWritten": 4331,
          "description": "Delta: Job group for statement 9:\ntable_name = 'dimension_country'\ndimension_country_schema = StructType([\n    StructField('ID', IntegerType(), True),\n    StructField('Country', StringType(), True), \n    StructField('Region', StringType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_country_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 2",
          "displayName": "toString at String.java:2994",
          "jobGroup": "9",
          "jobId": 45,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 3,
          "numTasks": 53,
          "rowCount": 60,
          "stageIds": [
           81,
           82
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:47.768GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:47.649GMT",
          "dataRead": 3551,
          "dataWritten": 4772,
          "description": "Delta: Job group for statement 9:\ntable_name = 'dimension_country'\ndimension_country_schema = StructType([\n    StructField('ID', IntegerType(), True),\n    StructField('Country', StringType(), True), \n    StructField('Region', StringType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_country_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 2",
          "displayName": "toString at String.java:2994",
          "jobGroup": "9",
          "jobId": 44,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 3,
          "numCompletedStages": 1,
          "numCompletedTasks": 3,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 3,
          "rowCount": 20,
          "stageIds": [
           80
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:47.592GMT",
          "usageDescription": ""
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 9,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 9
      },
      "text/plain": [
       "StatementMeta(, 4259ac00-d937-4ed0-b39b-3a46ab7c1f3b, 9, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table_name = 'dimension_country'\n",
    "dimension_country_schema = StructType([\n",
    "    StructField('ID', IntegerType(), True),\n",
    "    StructField('Country', StringType(), True), \n",
    "    StructField('Region', StringType(), True)])\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(dimension_country_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n",
    "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\" + table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a089b2-97a1-4faf-8742-091576d76e79",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Dimension-City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd3b1d4a-1e40-4db1-9a23-67912a3f82d9",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-09-11T18:46:58.881065Z",
       "execution_start_time": "2023-09-11T18:46:52.4457885Z",
       "livy_statement_state": "available",
       "parent_msg_id": "84825c88-d503-4b88-8298-aa6a7c8e988a",
       "queued_time": "2023-09-11T18:46:16.2087048Z",
       "session_id": "4259ac00-d937-4ed0-b39b-3a46ab7c1f3b",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [
         {
          "completionTime": "2023-09-11T18:46:56.855GMT",
          "dataRead": 4479,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 10:\ntable_name = 'dimension_city'\ndimension_city_schema = StructType([\n    StructField('CityKey', IntegerType(), True),\n    StructField('CityID', IntegerType(), True), \n    StructField('City', StringType(), True),\n    StructField('StateProvince', StringType(), True),\n    StructField('Country', StringType(), True),\n    StructField('Continent', StringType(), True),\n    StructField('SalesTerritory', StringType(), True),\n    StructField('Region', StringType(), True),\n    StructField('SubRegion', StringType(), True),\n    StructField('Location', StringType(), True),\n    StructField('LatestRecordedPopulation', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "10",
          "jobId": 61,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 52,
          "numTasks": 53,
          "rowCount": 50,
          "stageIds": [
           111,
           112,
           113
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:56.829GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:56.812GMT",
          "dataRead": 3850,
          "dataWritten": 4479,
          "description": "Delta: Job group for statement 10:\ntable_name = 'dimension_city'\ndimension_city_schema = StructType([\n    StructField('CityKey', IntegerType(), True),\n    StructField('CityID', IntegerType(), True), \n    StructField('City', StringType(), True),\n    StructField('StateProvince', StringType(), True),\n    StructField('Country', StringType(), True),\n    StructField('Continent', StringType(), True),\n    StructField('SalesTerritory', StringType(), True),\n    StructField('Region', StringType(), True),\n    StructField('SubRegion', StringType(), True),\n    StructField('Location', StringType(), True),\n    StructField('LatestRecordedPopulation', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "10",
          "jobId": 60,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 2,
          "numTasks": 52,
          "rowCount": 57,
          "stageIds": [
           109,
           110
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:56.433GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:56.309GMT",
          "dataRead": 5105,
          "dataWritten": 3850,
          "description": "Delta: Job group for statement 10:\ntable_name = 'dimension_city'\ndimension_city_schema = StructType([\n    StructField('CityKey', IntegerType(), True),\n    StructField('CityID', IntegerType(), True), \n    StructField('City', StringType(), True),\n    StructField('StateProvince', StringType(), True),\n    StructField('Country', StringType(), True),\n    StructField('Continent', StringType(), True),\n    StructField('SalesTerritory', StringType(), True),\n    StructField('Region', StringType(), True),\n    StructField('SubRegion', StringType(), True),\n    StructField('Location', StringType(), True),\n    StructField('LatestRecordedPopulation', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "10",
          "jobId": 59,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 2,
          "numCompletedStages": 1,
          "numCompletedTasks": 2,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 2,
          "rowCount": 14,
          "stageIds": [
           108
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:56.253GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:55.874GMT",
          "dataRead": 2268,
          "dataWritten": 0,
          "description": "Job group for statement 10:\ntable_name = 'dimension_city'\ndimension_city_schema = StructType([\n    StructField('CityKey', IntegerType(), True),\n    StructField('CityID', IntegerType(), True), \n    StructField('City', StringType(), True),\n    StructField('StateProvince', StringType(), True),\n    StructField('Country', StringType(), True),\n    StructField('Continent', StringType(), True),\n    StructField('SalesTerritory', StringType(), True),\n    StructField('Region', StringType(), True),\n    StructField('SubRegion', StringType(), True),\n    StructField('Location', StringType(), True),\n    StructField('LatestRecordedPopulation', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "10",
          "jobId": 58,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 51,
          "rowCount": 3,
          "stageIds": [
           107,
           106
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:55.760GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:55.650GMT",
          "dataRead": 6317148,
          "dataWritten": 2401969,
          "description": "Job group for statement 10:\ntable_name = 'dimension_city'\ndimension_city_schema = StructType([\n    StructField('CityKey', IntegerType(), True),\n    StructField('CityID', IntegerType(), True), \n    StructField('City', StringType(), True),\n    StructField('StateProvince', StringType(), True),\n    StructField('Country', StringType(), True),\n    StructField('Continent', StringType(), True),\n    StructField('SalesTerritory', StringType(), True),\n    StructField('Region', StringType(), True),\n    StructField('SubRegion', StringType(), True),\n    StructField('Location', StringType(), True),\n    StructField('LatestRecordedPopulation', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "10",
          "jobId": 57,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 5,
          "numTasks": 6,
          "rowCount": 232590,
          "stageIds": [
           104,
           105
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:54.604GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:54.548GMT",
          "dataRead": 20587379,
          "dataWritten": 6317148,
          "description": "Job group for statement 10:\ntable_name = 'dimension_city'\ndimension_city_schema = StructType([\n    StructField('CityKey', IntegerType(), True),\n    StructField('CityID', IntegerType(), True), \n    StructField('City', StringType(), True),\n    StructField('StateProvince', StringType(), True),\n    StructField('Country', StringType(), True),\n    StructField('Continent', StringType(), True),\n    StructField('SalesTerritory', StringType(), True),\n    StructField('Region', StringType(), True),\n    StructField('SubRegion', StringType(), True),\n    StructField('Location', StringType(), True),\n    StructField('LatestRecordedPopulation', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "10",
          "jobId": 56,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 5,
          "numCompletedStages": 1,
          "numCompletedTasks": 5,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 5,
          "rowCount": 232590,
          "stageIds": [
           103
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:53.910GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:53.642GMT",
          "dataRead": 4474,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 10:\ntable_name = 'dimension_city'\ndimension_city_schema = StructType([\n    StructField('CityKey', IntegerType(), True),\n    StructField('CityID', IntegerType(), True), \n    StructField('City', StringType(), True),\n    StructField('StateProvince', StringType(), True),\n    StructField('Country', StringType(), True),\n    StructField('Continent', StringType(), True),\n    StructField('SalesTerritory', StringType(), True),\n    StructField('Region', StringType(), True),\n    StructField('SubRegion', StringType(), True),\n    StructField('Location', StringType(), True),\n    StructField('LatestRecordedPopulation', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 0",
          "displayName": "toString at String.java:2994",
          "jobGroup": "10",
          "jobId": 55,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 51,
          "numTasks": 52,
          "rowCount": 50,
          "stageIds": [
           102,
           100,
           101
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:53.613GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:53.589GMT",
          "dataRead": 2055,
          "dataWritten": 4474,
          "description": "Delta: Job group for statement 10:\ntable_name = 'dimension_city'\ndimension_city_schema = StructType([\n    StructField('CityKey', IntegerType(), True),\n    StructField('CityID', IntegerType(), True), \n    StructField('City', StringType(), True),\n    StructField('StateProvince', StringType(), True),\n    StructField('Country', StringType(), True),\n    StructField('Continent', StringType(), True),\n    StructField('SalesTerritory', StringType(), True),\n    StructField('Region', StringType(), True),\n    StructField('SubRegion', StringType(), True),\n    StructField('Location', StringType(), True),\n    StructField('LatestRecordedPopulation', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 0",
          "displayName": "toString at String.java:2994",
          "jobGroup": "10",
          "jobId": 54,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 51,
          "rowCount": 54,
          "stageIds": [
           99,
           98
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:53.158GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:53.052GMT",
          "dataRead": 3095,
          "dataWritten": 2055,
          "description": "Delta: Job group for statement 10:\ntable_name = 'dimension_city'\ndimension_city_schema = StructType([\n    StructField('CityKey', IntegerType(), True),\n    StructField('CityID', IntegerType(), True), \n    StructField('City', StringType(), True),\n    StructField('StateProvince', StringType(), True),\n    StructField('Country', StringType(), True),\n    StructField('Continent', StringType(), True),\n    StructField('SalesTerritory', StringType(), True),\n    StructField('Region', StringType(), True),\n    StructField('SubRegion', StringType(), True),\n    StructField('Location', StringType(), True),\n    StructField('LatestRecordedPopulation', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 0",
          "displayName": "toString at String.java:2994",
          "jobGroup": "10",
          "jobId": 53,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 8,
          "stageIds": [
           97
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:53.003GMT",
          "usageDescription": ""
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 9,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 10
      },
      "text/plain": [
       "StatementMeta(, 4259ac00-d937-4ed0-b39b-3a46ab7c1f3b, 10, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table_name = 'dimension_city'\n",
    "dimension_city_schema = StructType([\n",
    "    StructField('CityKey', IntegerType(), True),\n",
    "    StructField('CityID', IntegerType(), True), \n",
    "    StructField('City', StringType(), True),\n",
    "    StructField('StateProvince', StringType(), True),\n",
    "    StructField('Country', StringType(), True),\n",
    "    StructField('Continent', StringType(), True),\n",
    "    StructField('SalesTerritory', StringType(), True),\n",
    "    StructField('Region', StringType(), True),\n",
    "    StructField('SubRegion', StringType(), True),\n",
    "    StructField('Location', StringType(), True),\n",
    "    StructField('LatestRecordedPopulation', StringType(), True),\n",
    "    StructField('ValidFrom', StringType(), True),\n",
    "    StructField('ValidTo', StringType(), True),\n",
    "    StructField('LineageKey', IntegerType(), True)])\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n",
    "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\"+table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d155bdcb-c674-4557-b045-cb772a07c509",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Dimension-Employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dde7f2e4-68c3-460c-94c2-6f807f747881",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-09-11T18:47:04.2205959Z",
       "execution_start_time": "2023-09-11T18:46:59.2978569Z",
       "livy_statement_state": "available",
       "parent_msg_id": "83949565-b474-48ad-a239-f69c684663dd",
       "queued_time": "2023-09-11T18:46:16.6806078Z",
       "session_id": "4259ac00-d937-4ed0-b39b-3a46ab7c1f3b",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [
         {
          "completionTime": "2023-09-11T18:47:02.271GMT",
          "dataRead": 4402,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 11:\ntable_name = 'dimension_employee'\ndimension_city_schema = StructType([\n    StructField('EmployeeKey', IntegerType(), True),\n    StructField('EmployeeID', IntegerType(), True), \n    StructField('EmployeeName', StringType(), True),\n    StructField('PreferredName', StringType(), True),\n    StructField('IsSalesPerson', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "11",
          "jobId": 70,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 52,
          "numTasks": 53,
          "rowCount": 50,
          "stageIds": [
           129,
           130,
           128
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:02.241GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:02.221GMT",
          "dataRead": 3583,
          "dataWritten": 4402,
          "description": "Delta: Job group for statement 11:\ntable_name = 'dimension_employee'\ndimension_city_schema = StructType([\n    StructField('EmployeeKey', IntegerType(), True),\n    StructField('EmployeeID', IntegerType(), True), \n    StructField('EmployeeName', StringType(), True),\n    StructField('PreferredName', StringType(), True),\n    StructField('IsSalesPerson', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "11",
          "jobId": 69,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 2,
          "numTasks": 52,
          "rowCount": 57,
          "stageIds": [
           126,
           127
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:01.802GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:01.686GMT",
          "dataRead": 3730,
          "dataWritten": 3583,
          "description": "Delta: Job group for statement 11:\ntable_name = 'dimension_employee'\ndimension_city_schema = StructType([\n    StructField('EmployeeKey', IntegerType(), True),\n    StructField('EmployeeID', IntegerType(), True), \n    StructField('EmployeeName', StringType(), True),\n    StructField('PreferredName', StringType(), True),\n    StructField('IsSalesPerson', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "11",
          "jobId": 68,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 2,
          "numCompletedStages": 1,
          "numCompletedTasks": 2,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 2,
          "rowCount": 14,
          "stageIds": [
           125
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:01.633GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:01.250GMT",
          "dataRead": 1947,
          "dataWritten": 0,
          "description": "Job group for statement 11:\ntable_name = 'dimension_employee'\ndimension_city_schema = StructType([\n    StructField('EmployeeKey', IntegerType(), True),\n    StructField('EmployeeID', IntegerType(), True), \n    StructField('EmployeeName', StringType(), True),\n    StructField('PreferredName', StringType(), True),\n    StructField('IsSalesPerson', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "11",
          "jobId": 67,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 51,
          "rowCount": 3,
          "stageIds": [
           123,
           124
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:01.156GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:01.057GMT",
          "dataRead": 7133,
          "dataWritten": 8467,
          "description": "Job group for statement 11:\ntable_name = 'dimension_employee'\ndimension_city_schema = StructType([\n    StructField('EmployeeKey', IntegerType(), True),\n    StructField('EmployeeID', IntegerType(), True), \n    StructField('EmployeeName', StringType(), True),\n    StructField('PreferredName', StringType(), True),\n    StructField('IsSalesPerson', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "11",
          "jobId": 66,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 2,
          "rowCount": 426,
          "stageIds": [
           121,
           122
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:00.801GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:00.765GMT",
          "dataRead": 16291,
          "dataWritten": 7133,
          "description": "Job group for statement 11:\ntable_name = 'dimension_employee'\ndimension_city_schema = StructType([\n    StructField('EmployeeKey', IntegerType(), True),\n    StructField('EmployeeID', IntegerType(), True), \n    StructField('EmployeeName', StringType(), True),\n    StructField('PreferredName', StringType(), True),\n    StructField('IsSalesPerson', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "11",
          "jobId": 65,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 426,
          "stageIds": [
           120
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:00.700GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:00.485GMT",
          "dataRead": 4397,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 11:\ntable_name = 'dimension_employee'\ndimension_city_schema = StructType([\n    StructField('EmployeeKey', IntegerType(), True),\n    StructField('EmployeeID', IntegerType(), True), \n    StructField('EmployeeName', StringType(), True),\n    StructField('PreferredName', StringType(), True),\n    StructField('IsSalesPerson', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 0",
          "displayName": "toString at String.java:2994",
          "jobGroup": "11",
          "jobId": 64,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 51,
          "numTasks": 52,
          "rowCount": 50,
          "stageIds": [
           117,
           118,
           119
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:00.462GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:00.448GMT",
          "dataRead": 1737,
          "dataWritten": 4397,
          "description": "Delta: Job group for statement 11:\ntable_name = 'dimension_employee'\ndimension_city_schema = StructType([\n    StructField('EmployeeKey', IntegerType(), True),\n    StructField('EmployeeID', IntegerType(), True), \n    StructField('EmployeeName', StringType(), True),\n    StructField('PreferredName', StringType(), True),\n    StructField('IsSalesPerson', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 0",
          "displayName": "toString at String.java:2994",
          "jobGroup": "11",
          "jobId": 63,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 51,
          "rowCount": 54,
          "stageIds": [
           115,
           116
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:59.976GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:46:59.861GMT",
          "dataRead": 2175,
          "dataWritten": 1737,
          "description": "Delta: Job group for statement 11:\ntable_name = 'dimension_employee'\ndimension_city_schema = StructType([\n    StructField('EmployeeKey', IntegerType(), True),\n    StructField('EmployeeID', IntegerType(), True), \n    StructField('EmployeeName', StringType(), True),\n    StructField('PreferredName', StringType(), True),\n    StructField('IsSalesPerson', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 0",
          "displayName": "toString at String.java:2994",
          "jobGroup": "11",
          "jobId": 62,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 8,
          "stageIds": [
           114
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:46:59.815GMT",
          "usageDescription": ""
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 9,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 11
      },
      "text/plain": [
       "StatementMeta(, 4259ac00-d937-4ed0-b39b-3a46ab7c1f3b, 11, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table_name = 'dimension_employee'\n",
    "dimension_city_schema = StructType([\n",
    "    StructField('EmployeeKey', IntegerType(), True),\n",
    "    StructField('EmployeeID', IntegerType(), True), \n",
    "    StructField('EmployeeName', StringType(), True),\n",
    "    StructField('PreferredName', StringType(), True),\n",
    "    StructField('IsSalesPerson', StringType(), True),\n",
    "    StructField('ValidFrom', StringType(), True),\n",
    "    StructField('ValidTo', StringType(), True),\n",
    "    StructField('LineageKey', IntegerType(), True)])\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n",
    "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\" + table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594540db-9e34-4856-a53f-03173a6deeb7",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Dimension-Paymentmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4ffff06-f6e1-4fea-9e69-d34b5878053b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-09-11T18:47:09.5475202Z",
       "execution_start_time": "2023-09-11T18:47:04.6426125Z",
       "livy_statement_state": "available",
       "parent_msg_id": "9bdd5be2-78b0-4b8b-aafd-d85004e23eea",
       "queued_time": "2023-09-11T18:46:17.2281324Z",
       "session_id": "4259ac00-d937-4ed0-b39b-3a46ab7c1f3b",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [
         {
          "completionTime": "2023-09-11T18:47:07.609GMT",
          "dataRead": 4373,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 12:\ntable_name = 'dimension_paymentmethod'\ndimension_city_schema = StructType([\n    StructField('PaymentMethodKey', IntegerType(), True),\n    StructField('PaymentMethodID', IntegerType(), True), \n    StructField('PaymentMethod', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "12",
          "jobId": 79,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 52,
          "numTasks": 53,
          "rowCount": 50,
          "stageIds": [
           147,
           145,
           146
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:07.586GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:07.569GMT",
          "dataRead": 3426,
          "dataWritten": 4373,
          "description": "Delta: Job group for statement 12:\ntable_name = 'dimension_paymentmethod'\ndimension_city_schema = StructType([\n    StructField('PaymentMethodKey', IntegerType(), True),\n    StructField('PaymentMethodID', IntegerType(), True), \n    StructField('PaymentMethod', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "12",
          "jobId": 78,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 2,
          "numTasks": 52,
          "rowCount": 57,
          "stageIds": [
           143,
           144
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:07.169GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:07.071GMT",
          "dataRead": 3315,
          "dataWritten": 3426,
          "description": "Delta: Job group for statement 12:\ntable_name = 'dimension_paymentmethod'\ndimension_city_schema = StructType([\n    StructField('PaymentMethodKey', IntegerType(), True),\n    StructField('PaymentMethodID', IntegerType(), True), \n    StructField('PaymentMethod', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "12",
          "jobId": 77,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 2,
          "numCompletedStages": 1,
          "numCompletedTasks": 2,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 2,
          "rowCount": 14,
          "stageIds": [
           142
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:06.994GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:06.602GMT",
          "dataRead": 1860,
          "dataWritten": 0,
          "description": "Job group for statement 12:\ntable_name = 'dimension_paymentmethod'\ndimension_city_schema = StructType([\n    StructField('PaymentMethodKey', IntegerType(), True),\n    StructField('PaymentMethodID', IntegerType(), True), \n    StructField('PaymentMethod', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "12",
          "jobId": 76,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 51,
          "rowCount": 3,
          "stageIds": [
           140,
           141
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:06.499GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:06.420GMT",
          "dataRead": 342,
          "dataWritten": 3351,
          "description": "Job group for statement 12:\ntable_name = 'dimension_paymentmethod'\ndimension_city_schema = StructType([\n    StructField('PaymentMethodKey', IntegerType(), True),\n    StructField('PaymentMethodID', IntegerType(), True), \n    StructField('PaymentMethod', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "12",
          "jobId": 75,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 2,
          "rowCount": 12,
          "stageIds": [
           139,
           138
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:06.143GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:06.103GMT",
          "dataRead": 441,
          "dataWritten": 342,
          "description": "Job group for statement 12:\ntable_name = 'dimension_paymentmethod'\ndimension_city_schema = StructType([\n    StructField('PaymentMethodKey', IntegerType(), True),\n    StructField('PaymentMethodID', IntegerType(), True), \n    StructField('PaymentMethod', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "12",
          "jobId": 74,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 12,
          "stageIds": [
           137
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:06.054GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:05.845GMT",
          "dataRead": 4368,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 12:\ntable_name = 'dimension_paymentmethod'\ndimension_city_schema = StructType([\n    StructField('PaymentMethodKey', IntegerType(), True),\n    StructField('PaymentMethodID', IntegerType(), True), \n    StructField('PaymentMethod', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 0",
          "displayName": "toString at String.java:2994",
          "jobGroup": "12",
          "jobId": 73,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 51,
          "numTasks": 52,
          "rowCount": 50,
          "stageIds": [
           135,
           136,
           134
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:05.816GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:05.803GMT",
          "dataRead": 1641,
          "dataWritten": 4368,
          "description": "Delta: Job group for statement 12:\ntable_name = 'dimension_paymentmethod'\ndimension_city_schema = StructType([\n    StructField('PaymentMethodKey', IntegerType(), True),\n    StructField('PaymentMethodID', IntegerType(), True), \n    StructField('PaymentMethod', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 0",
          "displayName": "toString at String.java:2994",
          "jobGroup": "12",
          "jobId": 72,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 51,
          "rowCount": 54,
          "stageIds": [
           132,
           133
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:05.390GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:05.249GMT",
          "dataRead": 1890,
          "dataWritten": 1641,
          "description": "Delta: Job group for statement 12:\ntable_name = 'dimension_paymentmethod'\ndimension_city_schema = StructType([\n    StructField('PaymentMethodKey', IntegerType(), True),\n    StructField('PaymentMethodID', IntegerType(), True), \n    StructField('PaymentMethod', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 0",
          "displayName": "toString at String.java:2994",
          "jobGroup": "12",
          "jobId": 71,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 8,
          "stageIds": [
           131
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:05.180GMT",
          "usageDescription": ""
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 9,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 12
      },
      "text/plain": [
       "StatementMeta(, 4259ac00-d937-4ed0-b39b-3a46ab7c1f3b, 12, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table_name = 'dimension_paymentmethod'\n",
    "dimension_city_schema = StructType([\n",
    "    StructField('PaymentMethodKey', IntegerType(), True),\n",
    "    StructField('PaymentMethodID', IntegerType(), True), \n",
    "    StructField('PaymentMethod', StringType(), True),\n",
    "    StructField('ValidFrom', StringType(), True),\n",
    "    StructField('ValidTo', StringType(), True),\n",
    "    StructField('LineageKey', IntegerType(), True)])\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n",
    "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\" + table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31db619d-7dbe-4cbf-830c-d73fa7dec7d6",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Dimension-Supplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "312c42ed-176d-46cd-b22d-21a3d784df50",
   "metadata": {
    "advisor": {
     "adviceMetadata": "{\"artifactId\":\"7eb7090e-e87e-4f18-86b5-44a0a1d259ed\",\"activityId\":\"4259ac00-d937-4ed0-b39b-3a46ab7c1f3b\",\"applicationId\":\"application_1694457631810_0001\",\"jobGroupId\":\"13\",\"advices\":{\"info\":1}}"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-09-11T18:47:14.8707622Z",
       "execution_start_time": "2023-09-11T18:47:09.934531Z",
       "livy_statement_state": "available",
       "parent_msg_id": "243024ba-8d0d-4efb-8f15-31bc64f6927d",
       "queued_time": "2023-09-11T18:46:17.579895Z",
       "session_id": "4259ac00-d937-4ed0-b39b-3a46ab7c1f3b",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [
         {
          "completionTime": "2023-09-11T18:47:12.846GMT",
          "dataRead": 4458,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 13:\ntable_name = 'dimension_supplier'\ndimension_city_schema = StructType([\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('SupplierID', IntegerType(), True), \n    StructField('Supplier', StringType(), True),\n    StructField('Category', StringType(), True),\n    StructField('PrimaryContact', StringType(), True),\n    StructField('SupplierReference', StringType(), True),\n    StructField('PaymentDays', IntegerType(), True),\n    StructField('PostalCode', IntegerType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "13",
          "jobId": 88,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 52,
          "numTasks": 53,
          "rowCount": 50,
          "stageIds": [
           162,
           163,
           164
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:12.823GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:12.806GMT",
          "dataRead": 3929,
          "dataWritten": 4458,
          "description": "Delta: Job group for statement 13:\ntable_name = 'dimension_supplier'\ndimension_city_schema = StructType([\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('SupplierID', IntegerType(), True), \n    StructField('Supplier', StringType(), True),\n    StructField('Category', StringType(), True),\n    StructField('PrimaryContact', StringType(), True),\n    StructField('SupplierReference', StringType(), True),\n    StructField('PaymentDays', IntegerType(), True),\n    StructField('PostalCode', IntegerType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "13",
          "jobId": 87,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 2,
          "numTasks": 52,
          "rowCount": 57,
          "stageIds": [
           161,
           160
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:12.439GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:12.340GMT",
          "dataRead": 4424,
          "dataWritten": 3929,
          "description": "Delta: Job group for statement 13:\ntable_name = 'dimension_supplier'\ndimension_city_schema = StructType([\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('SupplierID', IntegerType(), True), \n    StructField('Supplier', StringType(), True),\n    StructField('Category', StringType(), True),\n    StructField('PrimaryContact', StringType(), True),\n    StructField('SupplierReference', StringType(), True),\n    StructField('PaymentDays', IntegerType(), True),\n    StructField('PostalCode', IntegerType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "13",
          "jobId": 86,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 2,
          "numCompletedStages": 1,
          "numCompletedTasks": 2,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 2,
          "rowCount": 14,
          "stageIds": [
           159
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:12.293GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:11.918GMT",
          "dataRead": 2150,
          "dataWritten": 0,
          "description": "Job group for statement 13:\ntable_name = 'dimension_supplier'\ndimension_city_schema = StructType([\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('SupplierID', IntegerType(), True), \n    StructField('Supplier', StringType(), True),\n    StructField('Category', StringType(), True),\n    StructField('PrimaryContact', StringType(), True),\n    StructField('SupplierReference', StringType(), True),\n    StructField('PaymentDays', IntegerType(), True),\n    StructField('PostalCode', IntegerType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "13",
          "jobId": 85,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 51,
          "rowCount": 3,
          "stageIds": [
           157,
           158
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:11.824GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:11.754GMT",
          "dataRead": 2219,
          "dataWritten": 6704,
          "description": "Job group for statement 13:\ntable_name = 'dimension_supplier'\ndimension_city_schema = StructType([\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('SupplierID', IntegerType(), True), \n    StructField('Supplier', StringType(), True),\n    StructField('Category', StringType(), True),\n    StructField('PrimaryContact', StringType(), True),\n    StructField('SupplierReference', StringType(), True),\n    StructField('PaymentDays', IntegerType(), True),\n    StructField('PostalCode', IntegerType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "13",
          "jobId": 84,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 2,
          "rowCount": 56,
          "stageIds": [
           155,
           156
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:11.467GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:11.426GMT",
          "dataRead": 3542,
          "dataWritten": 2219,
          "description": "Job group for statement 13:\ntable_name = 'dimension_supplier'\ndimension_city_schema = StructType([\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('SupplierID', IntegerType(), True), \n    StructField('Supplier', StringType(), True),\n    StructField('Category', StringType(), True),\n    StructField('PrimaryContact', StringType(), True),\n    StructField('SupplierReference', StringType(), True),\n    StructField('PaymentDays', IntegerType(), True),\n    StructField('PostalCode', IntegerType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "13",
          "jobId": 83,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 56,
          "stageIds": [
           154
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:11.367GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:11.147GMT",
          "dataRead": 4453,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 13:\ntable_name = 'dimension_supplier'\ndimension_city_schema = StructType([\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('SupplierID', IntegerType(), True), \n    StructField('Supplier', StringType(), True),\n    StructField('Category', StringType(), True),\n    StructField('PrimaryContact', StringType(), True),\n    StructField('SupplierReference', StringType(), True),\n    StructField('PaymentDays', IntegerType(), True),\n    StructField('PostalCode', IntegerType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 0",
          "displayName": "toString at String.java:2994",
          "jobGroup": "13",
          "jobId": 82,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 51,
          "numTasks": 52,
          "rowCount": 50,
          "stageIds": [
           153,
           151,
           152
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:11.117GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:11.100GMT",
          "dataRead": 1939,
          "dataWritten": 4453,
          "description": "Delta: Job group for statement 13:\ntable_name = 'dimension_supplier'\ndimension_city_schema = StructType([\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('SupplierID', IntegerType(), True), \n    StructField('Supplier', StringType(), True),\n    StructField('Category', StringType(), True),\n    StructField('PrimaryContact', StringType(), True),\n    StructField('SupplierReference', StringType(), True),\n    StructField('PaymentDays', IntegerType(), True),\n    StructField('PostalCode', IntegerType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 0",
          "displayName": "toString at String.java:2994",
          "jobGroup": "13",
          "jobId": 81,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 51,
          "rowCount": 54,
          "stageIds": [
           150,
           149
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:10.679GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:10.578GMT",
          "dataRead": 2643,
          "dataWritten": 1939,
          "description": "Delta: Job group for statement 13:\ntable_name = 'dimension_supplier'\ndimension_city_schema = StructType([\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('SupplierID', IntegerType(), True), \n    StructField('Supplier', StringType(), True),\n    StructField('Category', StringType(), True),\n    StructField('PrimaryContact', StringType(), True),\n    StructField('SupplierReference', StringType(), True),\n    StructField('PaymentDays', IntegerType(), True),\n    StructField('PostalCode', IntegerType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 0",
          "displayName": "toString at String.java:2994",
          "jobGroup": "13",
          "jobId": 80,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 8,
          "stageIds": [
           148
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:10.524GMT",
          "usageDescription": ""
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 9,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 13
      },
      "text/plain": [
       "StatementMeta(, 4259ac00-d937-4ed0-b39b-3a46ab7c1f3b, 13, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table_name = 'dimension_supplier'\n",
    "dimension_city_schema = StructType([\n",
    "    StructField('SupplierKey', IntegerType(), True),\n",
    "    StructField('SupplierID', IntegerType(), True), \n",
    "    StructField('Supplier', StringType(), True),\n",
    "    StructField('Category', StringType(), True),\n",
    "    StructField('PrimaryContact', StringType(), True),\n",
    "    StructField('SupplierReference', StringType(), True),\n",
    "    StructField('PaymentDays', IntegerType(), True),\n",
    "    StructField('PostalCode', IntegerType(), True),\n",
    "    StructField('ValidFrom', StringType(), True),\n",
    "    StructField('ValidTO', StringType(), True),\n",
    "    StructField('LineageKey', IntegerType(), True)])\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n",
    "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\"+table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff28d396-42d9-4b31-9c9f-2431a9a37179",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Dimension-Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f733a0a-8f8f-4861-abff-53f578e1d840",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-09-11T18:47:18.8747902Z",
       "execution_start_time": "2023-09-11T18:47:15.2632379Z",
       "livy_statement_state": "available",
       "parent_msg_id": "ec81ba25-950d-4fe2-a77a-26ddb1d1a7ce",
       "queued_time": "2023-09-11T18:46:18.0554269Z",
       "session_id": "4259ac00-d937-4ed0-b39b-3a46ab7c1f3b",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [
         {
          "completionTime": "2023-09-11T18:47:18.211GMT",
          "dataRead": 4435,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 14:\ntable_name = 'dimension_product'\ndimension_product_schema = StructType([\n    StructField('Products_ID', IntegerType(), True), \n    StructField('ProductID', StringType(), True), \n    StructField('Name', StringType(), True),\n    StructField('Department', StringType(), True),\n    StructField('Category', StringType(), True),\n    StructField('SubCampaigns', StringType(), True),\n    StructField('TargetGender', StringType(), True),\n    StructField('TargetClassification', StringType(), True),\n    StructField('TargetGeneration', StringType(), True),\n    ] \n)\ndf = spark.read.format(\"csv\").schema(dimension_product_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 2",
          "displayName": "toString at String.java:2994",
          "jobGroup": "14",
          "jobId": 97,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 53,
          "numTasks": 54,
          "rowCount": 50,
          "stageIds": [
           179,
           180,
           181
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:18.181GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:18.165GMT",
          "dataRead": 5878,
          "dataWritten": 4435,
          "description": "Delta: Job group for statement 14:\ntable_name = 'dimension_product'\ndimension_product_schema = StructType([\n    StructField('Products_ID', IntegerType(), True), \n    StructField('ProductID', StringType(), True), \n    StructField('Name', StringType(), True),\n    StructField('Department', StringType(), True),\n    StructField('Category', StringType(), True),\n    StructField('SubCampaigns', StringType(), True),\n    StructField('TargetGender', StringType(), True),\n    StructField('TargetClassification', StringType(), True),\n    StructField('TargetGeneration', StringType(), True),\n    ] \n)\ndf = spark.read.format(\"csv\").schema(dimension_product_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 2",
          "displayName": "toString at String.java:2994",
          "jobGroup": "14",
          "jobId": 96,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 3,
          "numTasks": 53,
          "rowCount": 60,
          "stageIds": [
           177,
           178
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:17.785GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:17.695GMT",
          "dataRead": 5798,
          "dataWritten": 5878,
          "description": "Delta: Job group for statement 14:\ntable_name = 'dimension_product'\ndimension_product_schema = StructType([\n    StructField('Products_ID', IntegerType(), True), \n    StructField('ProductID', StringType(), True), \n    StructField('Name', StringType(), True),\n    StructField('Department', StringType(), True),\n    StructField('Category', StringType(), True),\n    StructField('SubCampaigns', StringType(), True),\n    StructField('TargetGender', StringType(), True),\n    StructField('TargetClassification', StringType(), True),\n    StructField('TargetGeneration', StringType(), True),\n    ] \n)\ndf = spark.read.format(\"csv\").schema(dimension_product_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 2",
          "displayName": "toString at String.java:2994",
          "jobGroup": "14",
          "jobId": 95,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 3,
          "numCompletedStages": 1,
          "numCompletedTasks": 3,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 3,
          "rowCount": 20,
          "stageIds": [
           176
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:17.608GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:17.193GMT",
          "dataRead": 2124,
          "dataWritten": 0,
          "description": "Job group for statement 14:\ntable_name = 'dimension_product'\ndimension_product_schema = StructType([\n    StructField('Products_ID', IntegerType(), True), \n    StructField('ProductID', StringType(), True), \n    StructField('Name', StringType(), True),\n    StructField('Department', StringType(), True),\n    StructField('Category', StringType(), True),\n    StructField('SubCampaigns', StringType(), True),\n    StructField('TargetGender', StringType(), True),\n    StructField('TargetClassification', StringType(), True),\n    StructField('TargetGeneration', StringType(), True),\n    ] \n)\ndf = spark.read.format(\"csv\").schema(dimension_product_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "14",
          "jobId": 94,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 2,
          "numTasks": 52,
          "rowCount": 3,
          "stageIds": [
           174,
           175
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:17.089GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:17.009GMT",
          "dataRead": 7489,
          "dataWritten": 8314,
          "description": "Job group for statement 14:\ntable_name = 'dimension_product'\ndimension_product_schema = StructType([\n    StructField('Products_ID', IntegerType(), True), \n    StructField('ProductID', StringType(), True), \n    StructField('Name', StringType(), True),\n    StructField('Department', StringType(), True),\n    StructField('Category', StringType(), True),\n    StructField('SubCampaigns', StringType(), True),\n    StructField('TargetGender', StringType(), True),\n    StructField('TargetClassification', StringType(), True),\n    StructField('TargetGeneration', StringType(), True),\n    ] \n)\ndf = spark.read.format(\"csv\").schema(dimension_product_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "14",
          "jobId": 93,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 2,
          "rowCount": 156,
          "stageIds": [
           172,
           173
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:16.750GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:16.704GMT",
          "dataRead": 9021,
          "dataWritten": 7489,
          "description": "Job group for statement 14:\ntable_name = 'dimension_product'\ndimension_product_schema = StructType([\n    StructField('Products_ID', IntegerType(), True), \n    StructField('ProductID', StringType(), True), \n    StructField('Name', StringType(), True),\n    StructField('Department', StringType(), True),\n    StructField('Category', StringType(), True),\n    StructField('SubCampaigns', StringType(), True),\n    StructField('TargetGender', StringType(), True),\n    StructField('TargetClassification', StringType(), True),\n    StructField('TargetGeneration', StringType(), True),\n    ] \n)\ndf = spark.read.format(\"csv\").schema(dimension_product_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "14",
          "jobId": 92,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 156,
          "stageIds": [
           171
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:16.655GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:16.437GMT",
          "dataRead": 4430,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 14:\ntable_name = 'dimension_product'\ndimension_product_schema = StructType([\n    StructField('Products_ID', IntegerType(), True), \n    StructField('ProductID', StringType(), True), \n    StructField('Name', StringType(), True),\n    StructField('Department', StringType(), True),\n    StructField('Category', StringType(), True),\n    StructField('SubCampaigns', StringType(), True),\n    StructField('TargetGender', StringType(), True),\n    StructField('TargetClassification', StringType(), True),\n    StructField('TargetGeneration', StringType(), True),\n    ] \n)\ndf = spark.read.format(\"csv\").schema(dimension_product_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "14",
          "jobId": 91,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 52,
          "numTasks": 53,
          "rowCount": 50,
          "stageIds": [
           168,
           169,
           170
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:16.414GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:16.398GMT",
          "dataRead": 3887,
          "dataWritten": 4430,
          "description": "Delta: Job group for statement 14:\ntable_name = 'dimension_product'\ndimension_product_schema = StructType([\n    StructField('Products_ID', IntegerType(), True), \n    StructField('ProductID', StringType(), True), \n    StructField('Name', StringType(), True),\n    StructField('Department', StringType(), True),\n    StructField('Category', StringType(), True),\n    StructField('SubCampaigns', StringType(), True),\n    StructField('TargetGender', StringType(), True),\n    StructField('TargetClassification', StringType(), True),\n    StructField('TargetGeneration', StringType(), True),\n    ] \n)\ndf = spark.read.format(\"csv\").schema(dimension_product_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "14",
          "jobId": 90,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 2,
          "numTasks": 52,
          "rowCount": 57,
          "stageIds": [
           166,
           167
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:15.970GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:15.851GMT",
          "dataRead": 4100,
          "dataWritten": 3887,
          "description": "Delta: Job group for statement 14:\ntable_name = 'dimension_product'\ndimension_product_schema = StructType([\n    StructField('Products_ID', IntegerType(), True), \n    StructField('ProductID', StringType(), True), \n    StructField('Name', StringType(), True),\n    StructField('Department', StringType(), True),\n    StructField('Category', StringType(), True),\n    StructField('SubCampaigns', StringType(), True),\n    StructField('TargetGender', StringType(), True),\n    StructField('TargetClassification', StringType(), True),\n    StructField('TargetGeneration', StringType(), True),\n    ] \n)\ndf = spark.read.format(\"csv\").schema(dimension_product_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "14",
          "jobId": 89,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 2,
          "numCompletedStages": 1,
          "numCompletedTasks": 2,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 2,
          "rowCount": 14,
          "stageIds": [
           165
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:15.804GMT",
          "usageDescription": ""
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 9,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 14
      },
      "text/plain": [
       "StatementMeta(, 4259ac00-d937-4ed0-b39b-3a46ab7c1f3b, 14, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table_name = 'dimension_product'\n",
    "dimension_product_schema = StructType([\n",
    "    StructField('Products_ID', IntegerType(), True), \n",
    "    StructField('ProductID', StringType(), True), \n",
    "    StructField('Name', StringType(), True),\n",
    "    StructField('Department', StringType(), True),\n",
    "    StructField('Category', StringType(), True),\n",
    "    StructField('SubCampaigns', StringType(), True),\n",
    "    StructField('TargetGender', StringType(), True),\n",
    "    StructField('TargetClassification', StringType(), True),\n",
    "    StructField('TargetGeneration', StringType(), True),\n",
    "    ] \n",
    ")\n",
    "df = spark.read.format(\"csv\").schema(dimension_product_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n",
    "\n",
    "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\"+table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a9d26d-f104-40d5-8f38-e7099be15ecd",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Dimension-Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4653cc3-59d5-4641-9e76-98884e7ae2de",
   "metadata": {
    "advisor": {
     "adviceMetadata": "{\"artifactId\":\"7eb7090e-e87e-4f18-86b5-44a0a1d259ed\",\"activityId\":\"4259ac00-d937-4ed0-b39b-3a46ab7c1f3b\",\"applicationId\":\"application_1694457631810_0001\",\"jobGroupId\":\"15\",\"advices\":{\"info\":1}}"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-09-11T18:47:24.1930123Z",
       "execution_start_time": "2023-09-11T18:47:19.2472721Z",
       "livy_statement_state": "available",
       "parent_msg_id": "f199a56d-41ec-4e69-889c-756e6b925943",
       "queued_time": "2023-09-11T18:46:18.7038189Z",
       "session_id": "4259ac00-d937-4ed0-b39b-3a46ab7c1f3b",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [
         {
          "completionTime": "2023-09-11T18:47:22.393GMT",
          "dataRead": 4425,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 15:\ntable_name = 'dim_date'\ndimension_city_schema = StructType([\n    StructField('Date', StringType(), True),\n    StructField('DayNumber', IntegerType(), True), \n    StructField('Day', IntegerType(), True),\n    StructField('MonthName', StringType(), True),\n    StructField('ShortMonthName', StringType(), True),\n    StructField('CYMonthNumber', IntegerType(), True),\n    StructField('CYMonthLabel', StringType(), True),\n    StructField('CYYear', IntegerType(), True),\n    StructField('CYYearLabel', StringType(), True),\n    StructField('FYMonthNumber', IntegerType(), True),\n    StructField('FYMonthLabel', StringType(), True),\n    StructField('FYYear', IntegerType(), True),\n    StructField('FYYearLabel', StringType(), True),\n    StructField('WeekNumber', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "15",
          "jobId": 106,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 52,
          "numTasks": 53,
          "rowCount": 50,
          "stageIds": [
           197,
           198,
           196
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:22.371GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:22.358GMT",
          "dataRead": 3751,
          "dataWritten": 4425,
          "description": "Delta: Job group for statement 15:\ntable_name = 'dim_date'\ndimension_city_schema = StructType([\n    StructField('Date', StringType(), True),\n    StructField('DayNumber', IntegerType(), True), \n    StructField('Day', IntegerType(), True),\n    StructField('MonthName', StringType(), True),\n    StructField('ShortMonthName', StringType(), True),\n    StructField('CYMonthNumber', IntegerType(), True),\n    StructField('CYMonthLabel', StringType(), True),\n    StructField('CYYear', IntegerType(), True),\n    StructField('CYYearLabel', StringType(), True),\n    StructField('FYMonthNumber', IntegerType(), True),\n    StructField('FYMonthLabel', StringType(), True),\n    StructField('FYYear', IntegerType(), True),\n    StructField('FYYearLabel', StringType(), True),\n    StructField('WeekNumber', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "15",
          "jobId": 105,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 2,
          "numTasks": 52,
          "rowCount": 57,
          "stageIds": [
           194,
           195
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:21.944GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:21.805GMT",
          "dataRead": 4745,
          "dataWritten": 3751,
          "description": "Delta: Job group for statement 15:\ntable_name = 'dim_date'\ndimension_city_schema = StructType([\n    StructField('Date', StringType(), True),\n    StructField('DayNumber', IntegerType(), True), \n    StructField('Day', IntegerType(), True),\n    StructField('MonthName', StringType(), True),\n    StructField('ShortMonthName', StringType(), True),\n    StructField('CYMonthNumber', IntegerType(), True),\n    StructField('CYMonthLabel', StringType(), True),\n    StructField('CYYear', IntegerType(), True),\n    StructField('CYYearLabel', StringType(), True),\n    StructField('FYMonthNumber', IntegerType(), True),\n    StructField('FYMonthLabel', StringType(), True),\n    StructField('FYYear', IntegerType(), True),\n    StructField('FYYearLabel', StringType(), True),\n    StructField('WeekNumber', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "15",
          "jobId": 104,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 2,
          "numCompletedStages": 1,
          "numCompletedTasks": 2,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 2,
          "rowCount": 14,
          "stageIds": [
           193
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:21.752GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:21.362GMT",
          "dataRead": 2100,
          "dataWritten": 0,
          "description": "Job group for statement 15:\ntable_name = 'dim_date'\ndimension_city_schema = StructType([\n    StructField('Date', StringType(), True),\n    StructField('DayNumber', IntegerType(), True), \n    StructField('Day', IntegerType(), True),\n    StructField('MonthName', StringType(), True),\n    StructField('ShortMonthName', StringType(), True),\n    StructField('CYMonthNumber', IntegerType(), True),\n    StructField('CYMonthLabel', StringType(), True),\n    StructField('CYYear', IntegerType(), True),\n    StructField('CYYearLabel', StringType(), True),\n    StructField('FYMonthNumber', IntegerType(), True),\n    StructField('FYMonthLabel', StringType(), True),\n    StructField('FYYear', IntegerType(), True),\n    StructField('FYYearLabel', StringType(), True),\n    StructField('WeekNumber', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "15",
          "jobId": 103,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 51,
          "rowCount": 3,
          "stageIds": [
           191,
           192
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:21.251GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:21.184GMT",
          "dataRead": 79740,
          "dataWritten": 37003,
          "description": "Job group for statement 15:\ntable_name = 'dim_date'\ndimension_city_schema = StructType([\n    StructField('Date', StringType(), True),\n    StructField('DayNumber', IntegerType(), True), \n    StructField('Day', IntegerType(), True),\n    StructField('MonthName', StringType(), True),\n    StructField('ShortMonthName', StringType(), True),\n    StructField('CYMonthNumber', IntegerType(), True),\n    StructField('CYMonthLabel', StringType(), True),\n    StructField('CYYear', IntegerType(), True),\n    StructField('CYYearLabel', StringType(), True),\n    StructField('FYMonthNumber', IntegerType(), True),\n    StructField('FYMonthLabel', StringType(), True),\n    StructField('FYYear', IntegerType(), True),\n    StructField('FYYearLabel', StringType(), True),\n    StructField('WeekNumber', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "15",
          "jobId": 102,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 2,
          "rowCount": 6574,
          "stageIds": [
           190,
           189
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:20.871GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:20.836GMT",
          "dataRead": 919441,
          "dataWritten": 79740,
          "description": "Job group for statement 15:\ntable_name = 'dim_date'\ndimension_city_schema = StructType([\n    StructField('Date', StringType(), True),\n    StructField('DayNumber', IntegerType(), True), \n    StructField('Day', IntegerType(), True),\n    StructField('MonthName', StringType(), True),\n    StructField('ShortMonthName', StringType(), True),\n    StructField('CYMonthNumber', IntegerType(), True),\n    StructField('CYMonthLabel', StringType(), True),\n    StructField('CYYear', IntegerType(), True),\n    StructField('CYYearLabel', StringType(), True),\n    StructField('FYMonthNumber', IntegerType(), True),\n    StructField('FYMonthLabel', StringType(), True),\n    StructField('FYYear', IntegerType(), True),\n    StructField('FYYearLabel', StringType(), True),\n    StructField('WeekNumber', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "15",
          "jobId": 101,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 6574,
          "stageIds": [
           188
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:20.640GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:20.375GMT",
          "dataRead": 4420,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 15:\ntable_name = 'dim_date'\ndimension_city_schema = StructType([\n    StructField('Date', StringType(), True),\n    StructField('DayNumber', IntegerType(), True), \n    StructField('Day', IntegerType(), True),\n    StructField('MonthName', StringType(), True),\n    StructField('ShortMonthName', StringType(), True),\n    StructField('CYMonthNumber', IntegerType(), True),\n    StructField('CYMonthLabel', StringType(), True),\n    StructField('CYYear', IntegerType(), True),\n    StructField('CYYearLabel', StringType(), True),\n    StructField('FYMonthNumber', IntegerType(), True),\n    StructField('FYMonthLabel', StringType(), True),\n    StructField('FYYear', IntegerType(), True),\n    StructField('FYYearLabel', StringType(), True),\n    StructField('WeekNumber', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 0",
          "displayName": "toString at String.java:2994",
          "jobGroup": "15",
          "jobId": 100,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 51,
          "numTasks": 52,
          "rowCount": 50,
          "stageIds": [
           186,
           187,
           185
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:20.351GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:20.334GMT",
          "dataRead": 1867,
          "dataWritten": 4420,
          "description": "Delta: Job group for statement 15:\ntable_name = 'dim_date'\ndimension_city_schema = StructType([\n    StructField('Date', StringType(), True),\n    StructField('DayNumber', IntegerType(), True), \n    StructField('Day', IntegerType(), True),\n    StructField('MonthName', StringType(), True),\n    StructField('ShortMonthName', StringType(), True),\n    StructField('CYMonthNumber', IntegerType(), True),\n    StructField('CYMonthLabel', StringType(), True),\n    StructField('CYYear', IntegerType(), True),\n    StructField('CYYearLabel', StringType(), True),\n    StructField('FYMonthNumber', IntegerType(), True),\n    StructField('FYMonthLabel', StringType(), True),\n    StructField('FYYear', IntegerType(), True),\n    StructField('FYYearLabel', StringType(), True),\n    StructField('WeekNumber', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 0",
          "displayName": "toString at String.java:2994",
          "jobGroup": "15",
          "jobId": 99,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 51,
          "rowCount": 54,
          "stageIds": [
           183,
           184
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:19.955GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:19.839GMT",
          "dataRead": 2956,
          "dataWritten": 1867,
          "description": "Delta: Job group for statement 15:\ntable_name = 'dim_date'\ndimension_city_schema = StructType([\n    StructField('Date', StringType(), True),\n    StructField('DayNumber', IntegerType(), True), \n    StructField('Day', IntegerType(), True),\n    StructField('MonthName', StringType(), True),\n    StructField('ShortMonthName', StringType(), True),\n    StructField('CYMonthNumber', IntegerType(), True),\n    StructField('CYMonthLabel', StringType(), True),\n    StructField('CYYear', IntegerType(), True),\n    StructField('CYYearLabel', StringType(), True),\n    StructField('FYMonthNumber', IntegerType(), True),\n    StructField('FYMonthLabel', StringType(), True),\n    StructField('FYYear', IntegerType(), True),\n    StructField('FYYearLabel', StringType(), True),\n    StructField('WeekNumber', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 0",
          "displayName": "toString at String.java:2994",
          "jobGroup": "15",
          "jobId": 98,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 8,
          "stageIds": [
           182
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:19.789GMT",
          "usageDescription": ""
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 9,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 15
      },
      "text/plain": [
       "StatementMeta(, 4259ac00-d937-4ed0-b39b-3a46ab7c1f3b, 15, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table_name = 'dim_date'\n",
    "dimension_city_schema = StructType([\n",
    "    StructField('Date', StringType(), True),\n",
    "    StructField('DayNumber', IntegerType(), True), \n",
    "    StructField('Day', IntegerType(), True),\n",
    "    StructField('MonthName', StringType(), True),\n",
    "    StructField('ShortMonthName', StringType(), True),\n",
    "    StructField('CYMonthNumber', IntegerType(), True),\n",
    "    StructField('CYMonthLabel', StringType(), True),\n",
    "    StructField('CYYear', IntegerType(), True),\n",
    "    StructField('CYYearLabel', StringType(), True),\n",
    "    StructField('FYMonthNumber', IntegerType(), True),\n",
    "    StructField('FYMonthLabel', StringType(), True),\n",
    "    StructField('FYYear', IntegerType(), True),\n",
    "    StructField('FYYearLabel', StringType(), True),\n",
    "    StructField('WeekNumber', IntegerType(), True)])\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n",
    "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\"+table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b202a94b-6cce-4d48-a47f-42e826cb5059",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Dimension-Stockitem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03a3c848-faec-4f92-bff7-0bd79ebf8361",
   "metadata": {
    "advisor": {
     "adviceMetadata": "{\"artifactId\":\"7eb7090e-e87e-4f18-86b5-44a0a1d259ed\",\"activityId\":\"4259ac00-d937-4ed0-b39b-3a46ab7c1f3b\",\"applicationId\":\"application_1694457631810_0001\",\"jobGroupId\":\"16\",\"advices\":{\"info\":1}}"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-09-11T18:47:29.5116013Z",
       "execution_start_time": "2023-09-11T18:47:24.6335064Z",
       "livy_statement_state": "available",
       "parent_msg_id": "ca4ba5fd-f0e8-47cd-a8aa-f02a34727b4a",
       "queued_time": "2023-09-11T18:46:19.1924945Z",
       "session_id": "4259ac00-d937-4ed0-b39b-3a46ab7c1f3b",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [
         {
          "completionTime": "2023-09-11T18:47:27.785GMT",
          "dataRead": 4573,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 16:\ntable_name = 'dimension_stockitem'\ndimension_city_schema = StructType([\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('StockItemID', IntegerType(), True), \n    StructField('StockItem', StringType(), True),\n    StructField('Color', StringType(), True),\n    StructField('SellingPackage', StringType(), True),\n    StructField('BuyingPackage', StringType(), True),\n    StructField('Brand', StringType(), True),\n    StructField('Size', StringType(), True),\n    StructField('LeadTimeDay', IntegerType(), True),\n    StructField('QuantityPerOuter', IntegerType(), True),\n    StructField('IsChillerStock', StringType(), True),\n    StructField('Barcode', StringType(), True),\n    StructField('TaxRate', IntegerType(), True),\n    StructField('UnitPrice', IntegerType(), True),\n    StructField('RecommendedRetailPrice', DecimalType(), True),\n    StructField('WeightPerUnit', DecimalType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('...: Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "16",
          "jobId": 115,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 52,
          "numTasks": 53,
          "rowCount": 50,
          "stageIds": [
           215,
           213,
           214
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:27.762GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:27.746GMT",
          "dataRead": 4543,
          "dataWritten": 4573,
          "description": "Delta: Job group for statement 16:\ntable_name = 'dimension_stockitem'\ndimension_city_schema = StructType([\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('StockItemID', IntegerType(), True), \n    StructField('StockItem', StringType(), True),\n    StructField('Color', StringType(), True),\n    StructField('SellingPackage', StringType(), True),\n    StructField('BuyingPackage', StringType(), True),\n    StructField('Brand', StringType(), True),\n    StructField('Size', StringType(), True),\n    StructField('LeadTimeDay', IntegerType(), True),\n    StructField('QuantityPerOuter', IntegerType(), True),\n    StructField('IsChillerStock', StringType(), True),\n    StructField('Barcode', StringType(), True),\n    StructField('TaxRate', IntegerType(), True),\n    StructField('UnitPrice', IntegerType(), True),\n    StructField('RecommendedRetailPrice', DecimalType(), True),\n    StructField('WeightPerUnit', DecimalType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('...: Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "16",
          "jobId": 114,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 2,
          "numTasks": 52,
          "rowCount": 57,
          "stageIds": [
           212,
           211
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:27.309GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:27.178GMT",
          "dataRead": 6104,
          "dataWritten": 4543,
          "description": "Delta: Job group for statement 16:\ntable_name = 'dimension_stockitem'\ndimension_city_schema = StructType([\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('StockItemID', IntegerType(), True), \n    StructField('StockItem', StringType(), True),\n    StructField('Color', StringType(), True),\n    StructField('SellingPackage', StringType(), True),\n    StructField('BuyingPackage', StringType(), True),\n    StructField('Brand', StringType(), True),\n    StructField('Size', StringType(), True),\n    StructField('LeadTimeDay', IntegerType(), True),\n    StructField('QuantityPerOuter', IntegerType(), True),\n    StructField('IsChillerStock', StringType(), True),\n    StructField('Barcode', StringType(), True),\n    StructField('TaxRate', IntegerType(), True),\n    StructField('UnitPrice', IntegerType(), True),\n    StructField('RecommendedRetailPrice', DecimalType(), True),\n    StructField('WeightPerUnit', DecimalType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('...: Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "16",
          "jobId": 113,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 2,
          "numCompletedStages": 1,
          "numCompletedTasks": 2,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 2,
          "rowCount": 14,
          "stageIds": [
           210
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:27.123GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:26.735GMT",
          "dataRead": 2511,
          "dataWritten": 0,
          "description": "Job group for statement 16:\ntable_name = 'dimension_stockitem'\ndimension_city_schema = StructType([\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('StockItemID', IntegerType(), True), \n    StructField('StockItem', StringType(), True),\n    StructField('Color', StringType(), True),\n    StructField('SellingPackage', StringType(), True),\n    StructField('BuyingPackage', StringType(), True),\n    StructField('Brand', StringType(), True),\n    StructField('Size', StringType(), True),\n    StructField('LeadTimeDay', IntegerType(), True),\n    StructField('QuantityPerOuter', IntegerType(), True),\n    StructField('IsChillerStock', StringType(), True),\n    StructField('Barcode', StringType(), True),\n    StructField('TaxRate', IntegerType(), True),\n    StructField('UnitPrice', IntegerType(), True),\n    StructField('RecommendedRetailPrice', DecimalType(), True),\n    StructField('WeightPerUnit', DecimalType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('...",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "16",
          "jobId": 112,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 51,
          "rowCount": 3,
          "stageIds": [
           208,
           209
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:26.633GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:26.571GMT",
          "dataRead": 23461,
          "dataWritten": 20932,
          "description": "Job group for statement 16:\ntable_name = 'dimension_stockitem'\ndimension_city_schema = StructType([\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('StockItemID', IntegerType(), True), \n    StructField('StockItem', StringType(), True),\n    StructField('Color', StringType(), True),\n    StructField('SellingPackage', StringType(), True),\n    StructField('BuyingPackage', StringType(), True),\n    StructField('Brand', StringType(), True),\n    StructField('Size', StringType(), True),\n    StructField('LeadTimeDay', IntegerType(), True),\n    StructField('QuantityPerOuter', IntegerType(), True),\n    StructField('IsChillerStock', StringType(), True),\n    StructField('Barcode', StringType(), True),\n    StructField('TaxRate', IntegerType(), True),\n    StructField('UnitPrice', IntegerType(), True),\n    StructField('RecommendedRetailPrice', DecimalType(), True),\n    StructField('WeightPerUnit', DecimalType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('...",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "16",
          "jobId": 111,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 2,
          "rowCount": 1344,
          "stageIds": [
           206,
           207
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:26.262GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:26.220GMT",
          "dataRead": 104282,
          "dataWritten": 23461,
          "description": "Job group for statement 16:\ntable_name = 'dimension_stockitem'\ndimension_city_schema = StructType([\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('StockItemID', IntegerType(), True), \n    StructField('StockItem', StringType(), True),\n    StructField('Color', StringType(), True),\n    StructField('SellingPackage', StringType(), True),\n    StructField('BuyingPackage', StringType(), True),\n    StructField('Brand', StringType(), True),\n    StructField('Size', StringType(), True),\n    StructField('LeadTimeDay', IntegerType(), True),\n    StructField('QuantityPerOuter', IntegerType(), True),\n    StructField('IsChillerStock', StringType(), True),\n    StructField('Barcode', StringType(), True),\n    StructField('TaxRate', IntegerType(), True),\n    StructField('UnitPrice', IntegerType(), True),\n    StructField('RecommendedRetailPrice', DecimalType(), True),\n    StructField('WeightPerUnit', DecimalType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('...",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "16",
          "jobId": 110,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 1344,
          "stageIds": [
           205
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:26.073GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:25.795GMT",
          "dataRead": 4568,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 16:\ntable_name = 'dimension_stockitem'\ndimension_city_schema = StructType([\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('StockItemID', IntegerType(), True), \n    StructField('StockItem', StringType(), True),\n    StructField('Color', StringType(), True),\n    StructField('SellingPackage', StringType(), True),\n    StructField('BuyingPackage', StringType(), True),\n    StructField('Brand', StringType(), True),\n    StructField('Size', StringType(), True),\n    StructField('LeadTimeDay', IntegerType(), True),\n    StructField('QuantityPerOuter', IntegerType(), True),\n    StructField('IsChillerStock', StringType(), True),\n    StructField('Barcode', StringType(), True),\n    StructField('TaxRate', IntegerType(), True),\n    StructField('UnitPrice', IntegerType(), True),\n    StructField('RecommendedRetailPrice', DecimalType(), True),\n    StructField('WeightPerUnit', DecimalType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('...: Compute snapshot for version: 0",
          "displayName": "toString at String.java:2994",
          "jobGroup": "16",
          "jobId": 109,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 51,
          "numTasks": 52,
          "rowCount": 50,
          "stageIds": [
           204,
           202,
           203
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:25.769GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:25.756GMT",
          "dataRead": 2303,
          "dataWritten": 4568,
          "description": "Delta: Job group for statement 16:\ntable_name = 'dimension_stockitem'\ndimension_city_schema = StructType([\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('StockItemID', IntegerType(), True), \n    StructField('StockItem', StringType(), True),\n    StructField('Color', StringType(), True),\n    StructField('SellingPackage', StringType(), True),\n    StructField('BuyingPackage', StringType(), True),\n    StructField('Brand', StringType(), True),\n    StructField('Size', StringType(), True),\n    StructField('LeadTimeDay', IntegerType(), True),\n    StructField('QuantityPerOuter', IntegerType(), True),\n    StructField('IsChillerStock', StringType(), True),\n    StructField('Barcode', StringType(), True),\n    StructField('TaxRate', IntegerType(), True),\n    StructField('UnitPrice', IntegerType(), True),\n    StructField('RecommendedRetailPrice', DecimalType(), True),\n    StructField('WeightPerUnit', DecimalType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('...: Compute snapshot for version: 0",
          "displayName": "toString at String.java:2994",
          "jobGroup": "16",
          "jobId": 108,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 51,
          "rowCount": 54,
          "stageIds": [
           201,
           200
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:25.327GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:25.221GMT",
          "dataRead": 3812,
          "dataWritten": 2303,
          "description": "Delta: Job group for statement 16:\ntable_name = 'dimension_stockitem'\ndimension_city_schema = StructType([\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('StockItemID', IntegerType(), True), \n    StructField('StockItem', StringType(), True),\n    StructField('Color', StringType(), True),\n    StructField('SellingPackage', StringType(), True),\n    StructField('BuyingPackage', StringType(), True),\n    StructField('Brand', StringType(), True),\n    StructField('Size', StringType(), True),\n    StructField('LeadTimeDay', IntegerType(), True),\n    StructField('QuantityPerOuter', IntegerType(), True),\n    StructField('IsChillerStock', StringType(), True),\n    StructField('Barcode', StringType(), True),\n    StructField('TaxRate', IntegerType(), True),\n    StructField('UnitPrice', IntegerType(), True),\n    StructField('RecommendedRetailPrice', DecimalType(), True),\n    StructField('WeightPerUnit', DecimalType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('...: Compute snapshot for version: 0",
          "displayName": "toString at String.java:2994",
          "jobGroup": "16",
          "jobId": 107,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 8,
          "stageIds": [
           199
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:25.177GMT",
          "usageDescription": ""
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 9,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 16
      },
      "text/plain": [
       "StatementMeta(, 4259ac00-d937-4ed0-b39b-3a46ab7c1f3b, 16, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table_name = 'dimension_stockitem'\n",
    "dimension_city_schema = StructType([\n",
    "    StructField('StockItemKey', IntegerType(), True),\n",
    "    StructField('StockItemID', IntegerType(), True), \n",
    "    StructField('StockItem', StringType(), True),\n",
    "    StructField('Color', StringType(), True),\n",
    "    StructField('SellingPackage', StringType(), True),\n",
    "    StructField('BuyingPackage', StringType(), True),\n",
    "    StructField('Brand', StringType(), True),\n",
    "    StructField('Size', StringType(), True),\n",
    "    StructField('LeadTimeDay', IntegerType(), True),\n",
    "    StructField('QuantityPerOuter', IntegerType(), True),\n",
    "    StructField('IsChillerStock', StringType(), True),\n",
    "    StructField('Barcode', StringType(), True),\n",
    "    StructField('TaxRate', IntegerType(), True),\n",
    "    StructField('UnitPrice', IntegerType(), True),\n",
    "    StructField('RecommendedRetailPrice', DecimalType(), True),\n",
    "    StructField('WeightPerUnit', DecimalType(), True),\n",
    "    StructField('ValidFrom', StringType(), True),\n",
    "    StructField('ValidTO', StringType(), True),\n",
    "    StructField('LineageKey', IntegerType(), True)])\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n",
    "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\"+table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f748f1-adb6-4cad-81d4-0dfdd81b877b",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Dimension-TransactionType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "213fb8b4-cd56-442e-9fe1-2b164c5958d7",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-09-11T18:47:33.6366461Z",
       "execution_start_time": "2023-09-11T18:47:29.9647977Z",
       "livy_statement_state": "available",
       "parent_msg_id": "88752bfe-634c-4c60-859b-9ea9a087451e",
       "queued_time": "2023-09-11T18:46:19.6507143Z",
       "session_id": "4259ac00-d937-4ed0-b39b-3a46ab7c1f3b",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [
         {
          "completionTime": "2023-09-11T18:47:32.697GMT",
          "dataRead": 4374,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 17:\ntable_name = 'dimension_transactiontype'\ndimension_city_schema = StructType([\n    StructField('TransactionTypeKey', IntegerType(), True),\n    StructField('TransactionTypeID', IntegerType(), True), \n    StructField('TransactionType', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "17",
          "jobId": 124,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 52,
          "numTasks": 53,
          "rowCount": 50,
          "stageIds": [
           230,
           231,
           232
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:32.675GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:32.658GMT",
          "dataRead": 3436,
          "dataWritten": 4374,
          "description": "Delta: Job group for statement 17:\ntable_name = 'dimension_transactiontype'\ndimension_city_schema = StructType([\n    StructField('TransactionTypeKey', IntegerType(), True),\n    StructField('TransactionTypeID', IntegerType(), True), \n    StructField('TransactionType', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "17",
          "jobId": 123,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 2,
          "numTasks": 52,
          "rowCount": 57,
          "stageIds": [
           228,
           229
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:32.227GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:32.140GMT",
          "dataRead": 3369,
          "dataWritten": 3436,
          "description": "Delta: Job group for statement 17:\ntable_name = 'dimension_transactiontype'\ndimension_city_schema = StructType([\n    StructField('TransactionTypeKey', IntegerType(), True),\n    StructField('TransactionTypeID', IntegerType(), True), \n    StructField('TransactionType', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "17",
          "jobId": 122,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 2,
          "numCompletedStages": 1,
          "numCompletedTasks": 2,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 2,
          "rowCount": 14,
          "stageIds": [
           227
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:32.097GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:31.719GMT",
          "dataRead": 1872,
          "dataWritten": 0,
          "description": "Job group for statement 17:\ntable_name = 'dimension_transactiontype'\ndimension_city_schema = StructType([\n    StructField('TransactionTypeKey', IntegerType(), True),\n    StructField('TransactionTypeID', IntegerType(), True), \n    StructField('TransactionType', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "17",
          "jobId": 121,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 51,
          "rowCount": 3,
          "stageIds": [
           225,
           226
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:31.602GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:31.532GMT",
          "dataRead": 642,
          "dataWritten": 3615,
          "description": "Job group for statement 17:\ntable_name = 'dimension_transactiontype'\ndimension_city_schema = StructType([\n    StructField('TransactionTypeKey', IntegerType(), True),\n    StructField('TransactionTypeID', IntegerType(), True), \n    StructField('TransactionType', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "17",
          "jobId": 120,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 2,
          "rowCount": 30,
          "stageIds": [
           223,
           224
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:31.289GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:31.254GMT",
          "dataRead": 1128,
          "dataWritten": 642,
          "description": "Job group for statement 17:\ntable_name = 'dimension_transactiontype'\ndimension_city_schema = StructType([\n    StructField('TransactionTypeKey', IntegerType(), True),\n    StructField('TransactionTypeID', IntegerType(), True), \n    StructField('TransactionType', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "17",
          "jobId": 119,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 30,
          "stageIds": [
           222
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:31.218GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:30.967GMT",
          "dataRead": 4369,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 17:\ntable_name = 'dimension_transactiontype'\ndimension_city_schema = StructType([\n    StructField('TransactionTypeKey', IntegerType(), True),\n    StructField('TransactionTypeID', IntegerType(), True), \n    StructField('TransactionType', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 0",
          "displayName": "toString at String.java:2994",
          "jobGroup": "17",
          "jobId": 118,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 51,
          "numTasks": 52,
          "rowCount": 50,
          "stageIds": [
           219,
           220,
           221
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:30.939GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:30.921GMT",
          "dataRead": 1645,
          "dataWritten": 4369,
          "description": "Delta: Job group for statement 17:\ntable_name = 'dimension_transactiontype'\ndimension_city_schema = StructType([\n    StructField('TransactionTypeKey', IntegerType(), True),\n    StructField('TransactionTypeID', IntegerType(), True), \n    StructField('TransactionType', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 0",
          "displayName": "toString at String.java:2994",
          "jobGroup": "17",
          "jobId": 117,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 51,
          "rowCount": 54,
          "stageIds": [
           217,
           218
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:30.600GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:30.477GMT",
          "dataRead": 1920,
          "dataWritten": 1645,
          "description": "Delta: Job group for statement 17:\ntable_name = 'dimension_transactiontype'\ndimension_city_schema = StructType([\n    StructField('TransactionTypeKey', IntegerType(), True),\n    StructField('TransactionTypeID', IntegerType(), True), \n    StructField('TransactionType', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 0",
          "displayName": "toString at String.java:2994",
          "jobGroup": "17",
          "jobId": 116,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 8,
          "stageIds": [
           216
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:30.431GMT",
          "usageDescription": ""
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 9,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 17
      },
      "text/plain": [
       "StatementMeta(, 4259ac00-d937-4ed0-b39b-3a46ab7c1f3b, 17, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table_name = 'dimension_transactiontype'\n",
    "dimension_city_schema = StructType([\n",
    "    StructField('TransactionTypeKey', IntegerType(), True),\n",
    "    StructField('TransactionTypeID', IntegerType(), True), \n",
    "    StructField('TransactionType', StringType(), True),\n",
    "    StructField('ValidFrom', StringType(), True),\n",
    "    StructField('ValidTO', StringType(), True),\n",
    "    StructField('LineageKey', IntegerType(), True)])\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n",
    "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\"+table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dddfc9-5b3c-411d-a4c0-4659899bde97",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Fact-Movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01b27ccb-cde7-43c0-809a-6c2da79c7a63",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-09-11T18:47:38.9940608Z",
       "execution_start_time": "2023-09-11T18:47:34.0438783Z",
       "livy_statement_state": "available",
       "parent_msg_id": "a4ad1507-efb0-47a1-9867-ee714bd050dc",
       "queued_time": "2023-09-11T18:46:20.0927675Z",
       "session_id": "4259ac00-d937-4ed0-b39b-3a46ab7c1f3b",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [
         {
          "completionTime": "2023-09-11T18:47:37.779GMT",
          "dataRead": 4455,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 18:\ntable_name = 'fact_movement'\ndimension_city_schema = StructType([\n    StructField('MovementKey', IntegerType(), True),\n    StructField('DateKey', StringType(), True), \n    StructField('StockItemKey', IntegerType(), True),\n    StructField('CustomerKey', IntegerType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('TransactionTypeKey', IntegerType(), True),\n    StructField('StockItemTransactionID', IntegerType(), True),\n    StructField('InvoiceID', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('Quantity', IntegerType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "18",
          "jobId": 133,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 52,
          "numTasks": 53,
          "rowCount": 50,
          "stageIds": [
           248,
           249,
           247
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:37.737GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:37.667GMT",
          "dataRead": 3727,
          "dataWritten": 4455,
          "description": "Delta: Job group for statement 18:\ntable_name = 'fact_movement'\ndimension_city_schema = StructType([\n    StructField('MovementKey', IntegerType(), True),\n    StructField('DateKey', StringType(), True), \n    StructField('StockItemKey', IntegerType(), True),\n    StructField('CustomerKey', IntegerType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('TransactionTypeKey', IntegerType(), True),\n    StructField('StockItemTransactionID', IntegerType(), True),\n    StructField('InvoiceID', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('Quantity', IntegerType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "18",
          "jobId": 132,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 2,
          "numTasks": 52,
          "rowCount": 57,
          "stageIds": [
           245,
           246
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:37.213GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:37.109GMT",
          "dataRead": 4317,
          "dataWritten": 3727,
          "description": "Delta: Job group for statement 18:\ntable_name = 'fact_movement'\ndimension_city_schema = StructType([\n    StructField('MovementKey', IntegerType(), True),\n    StructField('DateKey', StringType(), True), \n    StructField('StockItemKey', IntegerType(), True),\n    StructField('CustomerKey', IntegerType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('TransactionTypeKey', IntegerType(), True),\n    StructField('StockItemTransactionID', IntegerType(), True),\n    StructField('InvoiceID', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('Quantity', IntegerType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "18",
          "jobId": 131,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 2,
          "numCompletedStages": 1,
          "numCompletedTasks": 2,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 2,
          "rowCount": 14,
          "stageIds": [
           244
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:37.064GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:36.659GMT",
          "dataRead": 2066,
          "dataWritten": 0,
          "description": "Job group for statement 18:\ntable_name = 'fact_movement'\ndimension_city_schema = StructType([\n    StructField('MovementKey', IntegerType(), True),\n    StructField('DateKey', StringType(), True), \n    StructField('StockItemKey', IntegerType(), True),\n    StructField('CustomerKey', IntegerType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('TransactionTypeKey', IntegerType(), True),\n    StructField('StockItemTransactionID', IntegerType(), True),\n    StructField('InvoiceID', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('Quantity', IntegerType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "18",
          "jobId": 130,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 51,
          "rowCount": 3,
          "stageIds": [
           242,
           243
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:36.565GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:36.490GMT",
          "dataRead": 7105164,
          "dataWritten": 3275587,
          "description": "Job group for statement 18:\ntable_name = 'fact_movement'\ndimension_city_schema = StructType([\n    StructField('MovementKey', IntegerType(), True),\n    StructField('DateKey', StringType(), True), \n    StructField('StockItemKey', IntegerType(), True),\n    StructField('CustomerKey', IntegerType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('TransactionTypeKey', IntegerType(), True),\n    StructField('StockItemTransactionID', IntegerType(), True),\n    StructField('InvoiceID', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('Quantity', IntegerType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "18",
          "jobId": 129,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 4,
          "numTasks": 5,
          "rowCount": 473334,
          "stageIds": [
           241,
           240
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:35.758GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:35.718GMT",
          "dataRead": 14445879,
          "dataWritten": 7105164,
          "description": "Job group for statement 18:\ntable_name = 'fact_movement'\ndimension_city_schema = StructType([\n    StructField('MovementKey', IntegerType(), True),\n    StructField('DateKey', StringType(), True), \n    StructField('StockItemKey', IntegerType(), True),\n    StructField('CustomerKey', IntegerType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('TransactionTypeKey', IntegerType(), True),\n    StructField('StockItemTransactionID', IntegerType(), True),\n    StructField('InvoiceID', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('Quantity', IntegerType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "18",
          "jobId": 128,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 4,
          "numCompletedStages": 1,
          "numCompletedTasks": 4,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 4,
          "rowCount": 473334,
          "stageIds": [
           239
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:35.349GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:35.136GMT",
          "dataRead": 4450,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 18:\ntable_name = 'fact_movement'\ndimension_city_schema = StructType([\n    StructField('MovementKey', IntegerType(), True),\n    StructField('DateKey', StringType(), True), \n    StructField('StockItemKey', IntegerType(), True),\n    StructField('CustomerKey', IntegerType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('TransactionTypeKey', IntegerType(), True),\n    StructField('StockItemTransactionID', IntegerType(), True),\n    StructField('InvoiceID', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('Quantity', IntegerType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 0",
          "displayName": "toString at String.java:2994",
          "jobGroup": "18",
          "jobId": 127,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 51,
          "numTasks": 52,
          "rowCount": 50,
          "stageIds": [
           237,
           238,
           236
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:35.114GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:35.098GMT",
          "dataRead": 1838,
          "dataWritten": 4450,
          "description": "Delta: Job group for statement 18:\ntable_name = 'fact_movement'\ndimension_city_schema = StructType([\n    StructField('MovementKey', IntegerType(), True),\n    StructField('DateKey', StringType(), True), \n    StructField('StockItemKey', IntegerType(), True),\n    StructField('CustomerKey', IntegerType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('TransactionTypeKey', IntegerType(), True),\n    StructField('StockItemTransactionID', IntegerType(), True),\n    StructField('InvoiceID', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('Quantity', IntegerType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 0",
          "displayName": "toString at String.java:2994",
          "jobGroup": "18",
          "jobId": 126,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 51,
          "rowCount": 54,
          "stageIds": [
           234,
           235
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:34.690GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:34.590GMT",
          "dataRead": 2600,
          "dataWritten": 1838,
          "description": "Delta: Job group for statement 18:\ntable_name = 'fact_movement'\ndimension_city_schema = StructType([\n    StructField('MovementKey', IntegerType(), True),\n    StructField('DateKey', StringType(), True), \n    StructField('StockItemKey', IntegerType(), True),\n    StructField('CustomerKey', IntegerType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('TransactionTypeKey', IntegerType(), True),\n    StructField('StockItemTransactionID', IntegerType(), True),\n    StructField('InvoiceID', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('Quantity', IntegerType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 0",
          "displayName": "toString at String.java:2994",
          "jobGroup": "18",
          "jobId": 125,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 8,
          "stageIds": [
           233
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:34.547GMT",
          "usageDescription": ""
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 9,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 18
      },
      "text/plain": [
       "StatementMeta(, 4259ac00-d937-4ed0-b39b-3a46ab7c1f3b, 18, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table_name = 'fact_movement'\n",
    "dimension_city_schema = StructType([\n",
    "    StructField('MovementKey', IntegerType(), True),\n",
    "    StructField('DateKey', StringType(), True), \n",
    "    StructField('StockItemKey', IntegerType(), True),\n",
    "    StructField('CustomerKey', IntegerType(), True),\n",
    "    StructField('SupplierKey', IntegerType(), True),\n",
    "    StructField('TransactionTypeKey', IntegerType(), True),\n",
    "    StructField('StockItemTransactionID', IntegerType(), True),\n",
    "    StructField('InvoiceID', IntegerType(), True),\n",
    "    StructField('PurchaseOrderID', IntegerType(), True),\n",
    "    StructField('Quantity', IntegerType(), True),\n",
    "    StructField('LineageKey', IntegerType(), True)])\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\n",
    "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\"+table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078dce73-e174-4be7-b87b-72a745047982",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Fact-Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0340fb62-a8f5-4176-8b5f-f12d75747edd",
   "metadata": {
    "advisor": {
     "adviceMetadata": "{\"artifactId\":\"7eb7090e-e87e-4f18-86b5-44a0a1d259ed\",\"activityId\":\"4259ac00-d937-4ed0-b39b-3a46ab7c1f3b\",\"applicationId\":\"application_1694457631810_0001\",\"jobGroupId\":\"19\",\"advices\":{\"info\":1}}"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-09-11T18:47:45.8985232Z",
       "execution_start_time": "2023-09-11T18:47:39.4070683Z",
       "livy_statement_state": "available",
       "parent_msg_id": "5c3bf667-3b1f-4c93-8204-738af761693e",
       "queued_time": "2023-09-11T18:46:20.425031Z",
       "session_id": "4259ac00-d937-4ed0-b39b-3a46ab7c1f3b",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [
         {
          "completionTime": "2023-09-11T18:47:43.631GMT",
          "dataRead": 4567,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 19:\ntable_name = 'fact_order'\ndimension_city_schema = StructType([\n    StructField('OrderKey', IntegerType(), True),\n    StructField('CityKey', IntegerType(), True), \n    StructField('CustomerKey', IntegerType(), True),\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('OrderDateKey', StringType(), True),\n    StructField('PickedDatekey', StringType(), True),\n    StructField('SalesPersonKey', IntegerType(), True),\n    StructField('PickerKey', IntegerType(), True),\n    StructField('OrderID', IntegerType(), True),\n    StructField('BackOrderID', IntegerType(), True),\n    StructField('Description', StringType(), True),\n    StructField('Package', StringType(), True),\n    StructField('Quantity', IntegerType(), True),\n    StructField('UnitPrice', IntegerType(), True),\n    StructField('TaxRate', IntegerType(), True),\n    StructField('TotalExcludingTax', IntegerType(), True),\n    StructField('TaxAmount', DecimalType(), True),\n    StructField('TotalIncludingTax', DecimalType(), True),\n    StructField('Li...: Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "19",
          "jobId": 142,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 52,
          "numTasks": 53,
          "rowCount": 50,
          "stageIds": [
           266,
           264,
           265
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:43.608GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:43.593GMT",
          "dataRead": 4408,
          "dataWritten": 4567,
          "description": "Delta: Job group for statement 19:\ntable_name = 'fact_order'\ndimension_city_schema = StructType([\n    StructField('OrderKey', IntegerType(), True),\n    StructField('CityKey', IntegerType(), True), \n    StructField('CustomerKey', IntegerType(), True),\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('OrderDateKey', StringType(), True),\n    StructField('PickedDatekey', StringType(), True),\n    StructField('SalesPersonKey', IntegerType(), True),\n    StructField('PickerKey', IntegerType(), True),\n    StructField('OrderID', IntegerType(), True),\n    StructField('BackOrderID', IntegerType(), True),\n    StructField('Description', StringType(), True),\n    StructField('Package', StringType(), True),\n    StructField('Quantity', IntegerType(), True),\n    StructField('UnitPrice', IntegerType(), True),\n    StructField('TaxRate', IntegerType(), True),\n    StructField('TotalExcludingTax', IntegerType(), True),\n    StructField('TaxAmount', DecimalType(), True),\n    StructField('TotalIncludingTax', DecimalType(), True),\n    StructField('Li...: Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "19",
          "jobId": 141,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 2,
          "numTasks": 52,
          "rowCount": 57,
          "stageIds": [
           263,
           262
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:43.227GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:43.143GMT",
          "dataRead": 6023,
          "dataWritten": 4408,
          "description": "Delta: Job group for statement 19:\ntable_name = 'fact_order'\ndimension_city_schema = StructType([\n    StructField('OrderKey', IntegerType(), True),\n    StructField('CityKey', IntegerType(), True), \n    StructField('CustomerKey', IntegerType(), True),\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('OrderDateKey', StringType(), True),\n    StructField('PickedDatekey', StringType(), True),\n    StructField('SalesPersonKey', IntegerType(), True),\n    StructField('PickerKey', IntegerType(), True),\n    StructField('OrderID', IntegerType(), True),\n    StructField('BackOrderID', IntegerType(), True),\n    StructField('Description', StringType(), True),\n    StructField('Package', StringType(), True),\n    StructField('Quantity', IntegerType(), True),\n    StructField('UnitPrice', IntegerType(), True),\n    StructField('TaxRate', IntegerType(), True),\n    StructField('TotalExcludingTax', IntegerType(), True),\n    StructField('TaxAmount', DecimalType(), True),\n    StructField('TotalIncludingTax', DecimalType(), True),\n    StructField('Li...: Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "19",
          "jobId": 140,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 2,
          "numCompletedStages": 1,
          "numCompletedTasks": 2,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 2,
          "rowCount": 14,
          "stageIds": [
           261
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:43.098GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:42.729GMT",
          "dataRead": 2458,
          "dataWritten": 0,
          "description": "Job group for statement 19:\ntable_name = 'fact_order'\ndimension_city_schema = StructType([\n    StructField('OrderKey', IntegerType(), True),\n    StructField('CityKey', IntegerType(), True), \n    StructField('CustomerKey', IntegerType(), True),\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('OrderDateKey', StringType(), True),\n    StructField('PickedDatekey', StringType(), True),\n    StructField('SalesPersonKey', IntegerType(), True),\n    StructField('PickerKey', IntegerType(), True),\n    StructField('OrderID', IntegerType(), True),\n    StructField('BackOrderID', IntegerType(), True),\n    StructField('Description', StringType(), True),\n    StructField('Package', StringType(), True),\n    StructField('Quantity', IntegerType(), True),\n    StructField('UnitPrice', IntegerType(), True),\n    StructField('TaxRate', IntegerType(), True),\n    StructField('TotalExcludingTax', IntegerType(), True),\n    StructField('TaxAmount', DecimalType(), True),\n    StructField('TotalIncludingTax', DecimalType(), True),\n    StructField('Li...",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "19",
          "jobId": 139,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 51,
          "rowCount": 3,
          "stageIds": [
           259,
           260
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:42.638GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:42.571GMT",
          "dataRead": 11525912,
          "dataWritten": 3543020,
          "description": "Job group for statement 19:\ntable_name = 'fact_order'\ndimension_city_schema = StructType([\n    StructField('OrderKey', IntegerType(), True),\n    StructField('CityKey', IntegerType(), True), \n    StructField('CustomerKey', IntegerType(), True),\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('OrderDateKey', StringType(), True),\n    StructField('PickedDatekey', StringType(), True),\n    StructField('SalesPersonKey', IntegerType(), True),\n    StructField('PickerKey', IntegerType(), True),\n    StructField('OrderID', IntegerType(), True),\n    StructField('BackOrderID', IntegerType(), True),\n    StructField('Description', StringType(), True),\n    StructField('Package', StringType(), True),\n    StructField('Quantity', IntegerType(), True),\n    StructField('UnitPrice', IntegerType(), True),\n    StructField('TaxRate', IntegerType(), True),\n    StructField('TotalExcludingTax', IntegerType(), True),\n    StructField('TaxAmount', DecimalType(), True),\n    StructField('TotalIncludingTax', DecimalType(), True),\n    StructField('Li...",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "19",
          "jobId": 138,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 8,
          "numTasks": 9,
          "rowCount": 462824,
          "stageIds": [
           257,
           258
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:41.490GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:41.448GMT",
          "dataRead": 35726826,
          "dataWritten": 11525912,
          "description": "Job group for statement 19:\ntable_name = 'fact_order'\ndimension_city_schema = StructType([\n    StructField('OrderKey', IntegerType(), True),\n    StructField('CityKey', IntegerType(), True), \n    StructField('CustomerKey', IntegerType(), True),\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('OrderDateKey', StringType(), True),\n    StructField('PickedDatekey', StringType(), True),\n    StructField('SalesPersonKey', IntegerType(), True),\n    StructField('PickerKey', IntegerType(), True),\n    StructField('OrderID', IntegerType(), True),\n    StructField('BackOrderID', IntegerType(), True),\n    StructField('Description', StringType(), True),\n    StructField('Package', StringType(), True),\n    StructField('Quantity', IntegerType(), True),\n    StructField('UnitPrice', IntegerType(), True),\n    StructField('TaxRate', IntegerType(), True),\n    StructField('TotalExcludingTax', IntegerType(), True),\n    StructField('TaxAmount', DecimalType(), True),\n    StructField('TotalIncludingTax', DecimalType(), True),\n    StructField('Li...",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "19",
          "jobId": 137,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 8,
          "numCompletedStages": 1,
          "numCompletedTasks": 8,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 8,
          "rowCount": 462824,
          "stageIds": [
           256
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:40.739GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:40.517GMT",
          "dataRead": 4562,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 19:\ntable_name = 'fact_order'\ndimension_city_schema = StructType([\n    StructField('OrderKey', IntegerType(), True),\n    StructField('CityKey', IntegerType(), True), \n    StructField('CustomerKey', IntegerType(), True),\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('OrderDateKey', StringType(), True),\n    StructField('PickedDatekey', StringType(), True),\n    StructField('SalesPersonKey', IntegerType(), True),\n    StructField('PickerKey', IntegerType(), True),\n    StructField('OrderID', IntegerType(), True),\n    StructField('BackOrderID', IntegerType(), True),\n    StructField('Description', StringType(), True),\n    StructField('Package', StringType(), True),\n    StructField('Quantity', IntegerType(), True),\n    StructField('UnitPrice', IntegerType(), True),\n    StructField('TaxRate', IntegerType(), True),\n    StructField('TotalExcludingTax', IntegerType(), True),\n    StructField('TaxAmount', DecimalType(), True),\n    StructField('TotalIncludingTax', DecimalType(), True),\n    StructField('Li...: Compute snapshot for version: 0",
          "displayName": "toString at String.java:2994",
          "jobGroup": "19",
          "jobId": 136,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 51,
          "numTasks": 52,
          "rowCount": 50,
          "stageIds": [
           255,
           253,
           254
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:40.495GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:40.478GMT",
          "dataRead": 2236,
          "dataWritten": 4562,
          "description": "Delta: Job group for statement 19:\ntable_name = 'fact_order'\ndimension_city_schema = StructType([\n    StructField('OrderKey', IntegerType(), True),\n    StructField('CityKey', IntegerType(), True), \n    StructField('CustomerKey', IntegerType(), True),\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('OrderDateKey', StringType(), True),\n    StructField('PickedDatekey', StringType(), True),\n    StructField('SalesPersonKey', IntegerType(), True),\n    StructField('PickerKey', IntegerType(), True),\n    StructField('OrderID', IntegerType(), True),\n    StructField('BackOrderID', IntegerType(), True),\n    StructField('Description', StringType(), True),\n    StructField('Package', StringType(), True),\n    StructField('Quantity', IntegerType(), True),\n    StructField('UnitPrice', IntegerType(), True),\n    StructField('TaxRate', IntegerType(), True),\n    StructField('TotalExcludingTax', IntegerType(), True),\n    StructField('TaxAmount', DecimalType(), True),\n    StructField('TotalIncludingTax', DecimalType(), True),\n    StructField('Li...: Compute snapshot for version: 0",
          "displayName": "toString at String.java:2994",
          "jobGroup": "19",
          "jobId": 135,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 51,
          "rowCount": 54,
          "stageIds": [
           251,
           252
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:40.109GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:39.990GMT",
          "dataRead": 3774,
          "dataWritten": 2236,
          "description": "Delta: Job group for statement 19:\ntable_name = 'fact_order'\ndimension_city_schema = StructType([\n    StructField('OrderKey', IntegerType(), True),\n    StructField('CityKey', IntegerType(), True), \n    StructField('CustomerKey', IntegerType(), True),\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('OrderDateKey', StringType(), True),\n    StructField('PickedDatekey', StringType(), True),\n    StructField('SalesPersonKey', IntegerType(), True),\n    StructField('PickerKey', IntegerType(), True),\n    StructField('OrderID', IntegerType(), True),\n    StructField('BackOrderID', IntegerType(), True),\n    StructField('Description', StringType(), True),\n    StructField('Package', StringType(), True),\n    StructField('Quantity', IntegerType(), True),\n    StructField('UnitPrice', IntegerType(), True),\n    StructField('TaxRate', IntegerType(), True),\n    StructField('TotalExcludingTax', IntegerType(), True),\n    StructField('TaxAmount', DecimalType(), True),\n    StructField('TotalIncludingTax', DecimalType(), True),\n    StructField('Li...: Compute snapshot for version: 0",
          "displayName": "toString at String.java:2994",
          "jobGroup": "19",
          "jobId": 134,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 8,
          "stageIds": [
           250
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:39.947GMT",
          "usageDescription": ""
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 9,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 19
      },
      "text/plain": [
       "StatementMeta(, 4259ac00-d937-4ed0-b39b-3a46ab7c1f3b, 19, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table_name = 'fact_order'\n",
    "dimension_city_schema = StructType([\n",
    "    StructField('OrderKey', IntegerType(), True),\n",
    "    StructField('CityKey', IntegerType(), True), \n",
    "    StructField('CustomerKey', IntegerType(), True),\n",
    "    StructField('StockItemKey', IntegerType(), True),\n",
    "    StructField('OrderDateKey', StringType(), True),\n",
    "    StructField('PickedDatekey', StringType(), True),\n",
    "    StructField('SalesPersonKey', IntegerType(), True),\n",
    "    StructField('PickerKey', IntegerType(), True),\n",
    "    StructField('OrderID', IntegerType(), True),\n",
    "    StructField('BackOrderID', IntegerType(), True),\n",
    "    StructField('Description', StringType(), True),\n",
    "    StructField('Package', StringType(), True),\n",
    "    StructField('Quantity', IntegerType(), True),\n",
    "    StructField('UnitPrice', IntegerType(), True),\n",
    "    StructField('TaxRate', IntegerType(), True),\n",
    "    StructField('TotalExcludingTax', IntegerType(), True),\n",
    "    StructField('TaxAmount', DecimalType(), True),\n",
    "    StructField('TotalIncludingTax', DecimalType(), True),\n",
    "    StructField('LineageKey', IntegerType(), True)])\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\n",
    "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\"+table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bba9a0-a265-4b9f-850d-e44566080aed",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Fact-Purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f058b472-63c6-4eda-81a4-ffb544f084bc",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-09-11T18:47:49.9977206Z",
       "execution_start_time": "2023-09-11T18:47:46.358715Z",
       "livy_statement_state": "available",
       "parent_msg_id": "3714c26f-ec9f-4554-b2b6-875f2995f642",
       "queued_time": "2023-09-11T18:46:20.8820769Z",
       "session_id": "4259ac00-d937-4ed0-b39b-3a46ab7c1f3b",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [
         {
          "completionTime": "2023-09-11T18:47:49.061GMT",
          "dataRead": 4454,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 20:\ntable_name = 'fact_purchase'\ndimension_city_schema = StructType([\n    StructField('PurchaseKey', IntegerType(), True), \n    StructField('DateKey', StringType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('OrderedOuters', IntegerType(), True),\n    StructField('OrderedQuantity', IntegerType(), True),\n    StructField('ReceivedOuters', IntegerType(), True),\n  StructField('Package', StringType(), True),\n    StructField('IsOrderFinalized', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "20",
          "jobId": 151,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 52,
          "numTasks": 53,
          "rowCount": 50,
          "stageIds": [
           281,
           282,
           283
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:49.037GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:49.016GMT",
          "dataRead": 3780,
          "dataWritten": 4454,
          "description": "Delta: Job group for statement 20:\ntable_name = 'fact_purchase'\ndimension_city_schema = StructType([\n    StructField('PurchaseKey', IntegerType(), True), \n    StructField('DateKey', StringType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('OrderedOuters', IntegerType(), True),\n    StructField('OrderedQuantity', IntegerType(), True),\n    StructField('ReceivedOuters', IntegerType(), True),\n  StructField('Package', StringType(), True),\n    StructField('IsOrderFinalized', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "20",
          "jobId": 150,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 2,
          "numTasks": 52,
          "rowCount": 57,
          "stageIds": [
           279,
           280
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:48.681GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:48.585GMT",
          "dataRead": 4313,
          "dataWritten": 3780,
          "description": "Delta: Job group for statement 20:\ntable_name = 'fact_purchase'\ndimension_city_schema = StructType([\n    StructField('PurchaseKey', IntegerType(), True), \n    StructField('DateKey', StringType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('OrderedOuters', IntegerType(), True),\n    StructField('OrderedQuantity', IntegerType(), True),\n    StructField('ReceivedOuters', IntegerType(), True),\n  StructField('Package', StringType(), True),\n    StructField('IsOrderFinalized', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "20",
          "jobId": 149,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 2,
          "numCompletedStages": 1,
          "numCompletedTasks": 2,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 2,
          "rowCount": 14,
          "stageIds": [
           278
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:48.540GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:48.149GMT",
          "dataRead": 2088,
          "dataWritten": 0,
          "description": "Job group for statement 20:\ntable_name = 'fact_purchase'\ndimension_city_schema = StructType([\n    StructField('PurchaseKey', IntegerType(), True), \n    StructField('DateKey', StringType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('OrderedOuters', IntegerType(), True),\n    StructField('OrderedQuantity', IntegerType(), True),\n    StructField('ReceivedOuters', IntegerType(), True),\n  StructField('Package', StringType(), True),\n    StructField('IsOrderFinalized', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "20",
          "jobId": 148,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 51,
          "rowCount": 3,
          "stageIds": [
           277,
           276
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:48.047GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:47.969GMT",
          "dataRead": 239642,
          "dataWritten": 169481,
          "description": "Job group for statement 20:\ntable_name = 'fact_purchase'\ndimension_city_schema = StructType([\n    StructField('PurchaseKey', IntegerType(), True), \n    StructField('DateKey', StringType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('OrderedOuters', IntegerType(), True),\n    StructField('OrderedQuantity', IntegerType(), True),\n    StructField('ReceivedOuters', IntegerType(), True),\n  StructField('Package', StringType(), True),\n    StructField('IsOrderFinalized', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "20",
          "jobId": 147,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 2,
          "rowCount": 16734,
          "stageIds": [
           274,
           275
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:47.686GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:47.649GMT",
          "dataRead": 563447,
          "dataWritten": 239642,
          "description": "Job group for statement 20:\ntable_name = 'fact_purchase'\ndimension_city_schema = StructType([\n    StructField('PurchaseKey', IntegerType(), True), \n    StructField('DateKey', StringType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('OrderedOuters', IntegerType(), True),\n    StructField('OrderedQuantity', IntegerType(), True),\n    StructField('ReceivedOuters', IntegerType(), True),\n  StructField('Package', StringType(), True),\n    StructField('IsOrderFinalized', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "20",
          "jobId": 146,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 16734,
          "stageIds": [
           273
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:47.549GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:47.335GMT",
          "dataRead": 4449,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 20:\ntable_name = 'fact_purchase'\ndimension_city_schema = StructType([\n    StructField('PurchaseKey', IntegerType(), True), \n    StructField('DateKey', StringType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('OrderedOuters', IntegerType(), True),\n    StructField('OrderedQuantity', IntegerType(), True),\n    StructField('ReceivedOuters', IntegerType(), True),\n  StructField('Package', StringType(), True),\n    StructField('IsOrderFinalized', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 0",
          "displayName": "toString at String.java:2994",
          "jobGroup": "20",
          "jobId": 145,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 51,
          "numTasks": 52,
          "rowCount": 50,
          "stageIds": [
           270,
           271,
           272
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:47.308GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:47.280GMT",
          "dataRead": 1866,
          "dataWritten": 4449,
          "description": "Delta: Job group for statement 20:\ntable_name = 'fact_purchase'\ndimension_city_schema = StructType([\n    StructField('PurchaseKey', IntegerType(), True), \n    StructField('DateKey', StringType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('OrderedOuters', IntegerType(), True),\n    StructField('OrderedQuantity', IntegerType(), True),\n    StructField('ReceivedOuters', IntegerType(), True),\n  StructField('Package', StringType(), True),\n    StructField('IsOrderFinalized', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 0",
          "displayName": "toString at String.java:2994",
          "jobGroup": "20",
          "jobId": 144,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 51,
          "rowCount": 54,
          "stageIds": [
           269,
           268
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:46.949GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:46.854GMT",
          "dataRead": 2596,
          "dataWritten": 1866,
          "description": "Delta: Job group for statement 20:\ntable_name = 'fact_purchase'\ndimension_city_schema = StructType([\n    StructField('PurchaseKey', IntegerType(), True), \n    StructField('DateKey', StringType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('OrderedOuters', IntegerType(), True),\n    StructField('OrderedQuantity', IntegerType(), True),\n    StructField('ReceivedOuters', IntegerType(), True),\n  StructField('Package', StringType(), True),\n    StructField('IsOrderFinalized', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 0",
          "displayName": "toString at String.java:2994",
          "jobGroup": "20",
          "jobId": 143,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 8,
          "stageIds": [
           267
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:46.810GMT",
          "usageDescription": ""
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 9,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 20
      },
      "text/plain": [
       "StatementMeta(, 4259ac00-d937-4ed0-b39b-3a46ab7c1f3b, 20, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table_name = 'fact_purchase'\n",
    "dimension_city_schema = StructType([\n",
    "    StructField('PurchaseKey', IntegerType(), True), \n",
    "    StructField('DateKey', StringType(), True),\n",
    "    StructField('SupplierKey', IntegerType(), True),\n",
    "    StructField('StockItemKey', IntegerType(), True),\n",
    "    StructField('PurchaseOrderID', IntegerType(), True),\n",
    "    StructField('OrderedOuters', IntegerType(), True),\n",
    "    StructField('OrderedQuantity', IntegerType(), True),\n",
    "    StructField('ReceivedOuters', IntegerType(), True),\n",
    "  StructField('Package', StringType(), True),\n",
    "    StructField('IsOrderFinalized', StringType(), True),\n",
    "    StructField('LineageKey', IntegerType(), True)])\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\n",
    "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\"+table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abca51e-80a3-48eb-bc8d-e079f33a8982",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Fact-Transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e739a97-4659-47e8-b60b-460d777878f3",
   "metadata": {
    "advisor": {
     "adviceMetadata": "{\"artifactId\":\"7eb7090e-e87e-4f18-86b5-44a0a1d259ed\",\"activityId\":\"4259ac00-d937-4ed0-b39b-3a46ab7c1f3b\",\"applicationId\":\"application_1694457631810_0001\",\"jobGroupId\":\"21\",\"advices\":{\"info\":1}}"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-09-11T18:47:55.3698328Z",
       "execution_start_time": "2023-09-11T18:47:50.4035492Z",
       "livy_statement_state": "available",
       "parent_msg_id": "7baa1332-5d37-4bb3-998b-9d2992c7e2f1",
       "queued_time": "2023-09-11T18:46:21.2230752Z",
       "session_id": "4259ac00-d937-4ed0-b39b-3a46ab7c1f3b",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [
         {
          "completionTime": "2023-09-11T18:47:54.126GMT",
          "dataRead": 4582,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 21:\ntable_name = 'fact_transaction'\ndimension_city_schema = StructType([\n    StructField('TransactionKey', IntegerType(), True),\n    StructField('DateKey', StringType(), True), \n    StructField('CustomerKey', IntegerType(), True),\n    StructField('BillToCustomerKey', IntegerType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('TransactionTypekey', IntegerType(), True),\n    StructField('PaymentMethodKey', IntegerType(), True),\n    StructField('CustomerTransactionID', IntegerType(), True),\n    StructField('SupplierTransactionID', IntegerType(), True),\n    StructField('InvoiceID', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('SupplierInvoiceNumber', IntegerType(), True),\n    StructField('TotalExcludingTax', IntegerType(), True),\n    StructField('TaxAmount', IntegerType(), True),\n    StructField('TotalIncludingTax', DecimalType(), True),\n    StructField('OutstandingBalance', IntegerType(), True),\n    StructField('IsFinalized', StringType...: Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "21",
          "jobId": 160,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 52,
          "numTasks": 53,
          "rowCount": 50,
          "stageIds": [
           299,
           300,
           298
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:54.104GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:54.089GMT",
          "dataRead": 4315,
          "dataWritten": 4582,
          "description": "Delta: Job group for statement 21:\ntable_name = 'fact_transaction'\ndimension_city_schema = StructType([\n    StructField('TransactionKey', IntegerType(), True),\n    StructField('DateKey', StringType(), True), \n    StructField('CustomerKey', IntegerType(), True),\n    StructField('BillToCustomerKey', IntegerType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('TransactionTypekey', IntegerType(), True),\n    StructField('PaymentMethodKey', IntegerType(), True),\n    StructField('CustomerTransactionID', IntegerType(), True),\n    StructField('SupplierTransactionID', IntegerType(), True),\n    StructField('InvoiceID', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('SupplierInvoiceNumber', IntegerType(), True),\n    StructField('TotalExcludingTax', IntegerType(), True),\n    StructField('TaxAmount', IntegerType(), True),\n    StructField('TotalIncludingTax', DecimalType(), True),\n    StructField('OutstandingBalance', IntegerType(), True),\n    StructField('IsFinalized', StringType...: Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "21",
          "jobId": 159,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 2,
          "numTasks": 52,
          "rowCount": 57,
          "stageIds": [
           296,
           297
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:53.721GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:53.618GMT",
          "dataRead": 6134,
          "dataWritten": 4315,
          "description": "Delta: Job group for statement 21:\ntable_name = 'fact_transaction'\ndimension_city_schema = StructType([\n    StructField('TransactionKey', IntegerType(), True),\n    StructField('DateKey', StringType(), True), \n    StructField('CustomerKey', IntegerType(), True),\n    StructField('BillToCustomerKey', IntegerType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('TransactionTypekey', IntegerType(), True),\n    StructField('PaymentMethodKey', IntegerType(), True),\n    StructField('CustomerTransactionID', IntegerType(), True),\n    StructField('SupplierTransactionID', IntegerType(), True),\n    StructField('InvoiceID', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('SupplierInvoiceNumber', IntegerType(), True),\n    StructField('TotalExcludingTax', IntegerType(), True),\n    StructField('TaxAmount', IntegerType(), True),\n    StructField('TotalIncludingTax', DecimalType(), True),\n    StructField('OutstandingBalance', IntegerType(), True),\n    StructField('IsFinalized', StringType...: Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "21",
          "jobId": 158,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 2,
          "numCompletedStages": 1,
          "numCompletedTasks": 2,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 2,
          "rowCount": 14,
          "stageIds": [
           295
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:53.571GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:53.189GMT",
          "dataRead": 2423,
          "dataWritten": 0,
          "description": "Job group for statement 21:\ntable_name = 'fact_transaction'\ndimension_city_schema = StructType([\n    StructField('TransactionKey', IntegerType(), True),\n    StructField('DateKey', StringType(), True), \n    StructField('CustomerKey', IntegerType(), True),\n    StructField('BillToCustomerKey', IntegerType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('TransactionTypekey', IntegerType(), True),\n    StructField('PaymentMethodKey', IntegerType(), True),\n    StructField('CustomerTransactionID', IntegerType(), True),\n    StructField('SupplierTransactionID', IntegerType(), True),\n    StructField('InvoiceID', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('SupplierInvoiceNumber', IntegerType(), True),\n    StructField('TotalExcludingTax', IntegerType(), True),\n    StructField('TaxAmount', IntegerType(), True),\n    StructField('TotalIncludingTax', DecimalType(), True),\n    StructField('OutstandingBalance', IntegerType(), True),\n    StructField('IsFinalized', StringType...",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "21",
          "jobId": 157,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 51,
          "rowCount": 3,
          "stageIds": [
           293,
           294
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:53.101GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:53.038GMT",
          "dataRead": 2831045,
          "dataWritten": 2191227,
          "description": "Job group for statement 21:\ntable_name = 'fact_transaction'\ndimension_city_schema = StructType([\n    StructField('TransactionKey', IntegerType(), True),\n    StructField('DateKey', StringType(), True), \n    StructField('CustomerKey', IntegerType(), True),\n    StructField('BillToCustomerKey', IntegerType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('TransactionTypekey', IntegerType(), True),\n    StructField('PaymentMethodKey', IntegerType(), True),\n    StructField('CustomerTransactionID', IntegerType(), True),\n    StructField('SupplierTransactionID', IntegerType(), True),\n    StructField('InvoiceID', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('SupplierInvoiceNumber', IntegerType(), True),\n    StructField('TotalExcludingTax', IntegerType(), True),\n    StructField('TaxAmount', IntegerType(), True),\n    StructField('TotalIncludingTax', DecimalType(), True),\n    StructField('OutstandingBalance', IntegerType(), True),\n    StructField('IsFinalized', StringType...",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "21",
          "jobId": 156,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 2,
          "numTasks": 3,
          "rowCount": 199170,
          "stageIds": [
           291,
           292
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:52.154GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:52.119GMT",
          "dataRead": 8166221,
          "dataWritten": 2831045,
          "description": "Job group for statement 21:\ntable_name = 'fact_transaction'\ndimension_city_schema = StructType([\n    StructField('TransactionKey', IntegerType(), True),\n    StructField('DateKey', StringType(), True), \n    StructField('CustomerKey', IntegerType(), True),\n    StructField('BillToCustomerKey', IntegerType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('TransactionTypekey', IntegerType(), True),\n    StructField('PaymentMethodKey', IntegerType(), True),\n    StructField('CustomerTransactionID', IntegerType(), True),\n    StructField('SupplierTransactionID', IntegerType(), True),\n    StructField('InvoiceID', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('SupplierInvoiceNumber', IntegerType(), True),\n    StructField('TotalExcludingTax', IntegerType(), True),\n    StructField('TaxAmount', IntegerType(), True),\n    StructField('TotalIncludingTax', DecimalType(), True),\n    StructField('OutstandingBalance', IntegerType(), True),\n    StructField('IsFinalized', StringType...",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "21",
          "jobId": 155,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 2,
          "numCompletedStages": 1,
          "numCompletedTasks": 2,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 2,
          "rowCount": 199170,
          "stageIds": [
           290
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:51.619GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:51.410GMT",
          "dataRead": 4577,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 21:\ntable_name = 'fact_transaction'\ndimension_city_schema = StructType([\n    StructField('TransactionKey', IntegerType(), True),\n    StructField('DateKey', StringType(), True), \n    StructField('CustomerKey', IntegerType(), True),\n    StructField('BillToCustomerKey', IntegerType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('TransactionTypekey', IntegerType(), True),\n    StructField('PaymentMethodKey', IntegerType(), True),\n    StructField('CustomerTransactionID', IntegerType(), True),\n    StructField('SupplierTransactionID', IntegerType(), True),\n    StructField('InvoiceID', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('SupplierInvoiceNumber', IntegerType(), True),\n    StructField('TotalExcludingTax', IntegerType(), True),\n    StructField('TaxAmount', IntegerType(), True),\n    StructField('TotalIncludingTax', DecimalType(), True),\n    StructField('OutstandingBalance', IntegerType(), True),\n    StructField('IsFinalized', StringType...: Compute snapshot for version: 0",
          "displayName": "toString at String.java:2994",
          "jobGroup": "21",
          "jobId": 154,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 51,
          "numTasks": 52,
          "rowCount": 50,
          "stageIds": [
           287,
           288,
           289
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:51.388GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:51.373GMT",
          "dataRead": 2193,
          "dataWritten": 4577,
          "description": "Delta: Job group for statement 21:\ntable_name = 'fact_transaction'\ndimension_city_schema = StructType([\n    StructField('TransactionKey', IntegerType(), True),\n    StructField('DateKey', StringType(), True), \n    StructField('CustomerKey', IntegerType(), True),\n    StructField('BillToCustomerKey', IntegerType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('TransactionTypekey', IntegerType(), True),\n    StructField('PaymentMethodKey', IntegerType(), True),\n    StructField('CustomerTransactionID', IntegerType(), True),\n    StructField('SupplierTransactionID', IntegerType(), True),\n    StructField('InvoiceID', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('SupplierInvoiceNumber', IntegerType(), True),\n    StructField('TotalExcludingTax', IntegerType(), True),\n    StructField('TaxAmount', IntegerType(), True),\n    StructField('TotalIncludingTax', DecimalType(), True),\n    StructField('OutstandingBalance', IntegerType(), True),\n    StructField('IsFinalized', StringType...: Compute snapshot for version: 0",
          "displayName": "toString at String.java:2994",
          "jobGroup": "21",
          "jobId": 153,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 51,
          "rowCount": 54,
          "stageIds": [
           285,
           286
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:51.042GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:50.943GMT",
          "dataRead": 3824,
          "dataWritten": 2193,
          "description": "Delta: Job group for statement 21:\ntable_name = 'fact_transaction'\ndimension_city_schema = StructType([\n    StructField('TransactionKey', IntegerType(), True),\n    StructField('DateKey', StringType(), True), \n    StructField('CustomerKey', IntegerType(), True),\n    StructField('BillToCustomerKey', IntegerType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('TransactionTypekey', IntegerType(), True),\n    StructField('PaymentMethodKey', IntegerType(), True),\n    StructField('CustomerTransactionID', IntegerType(), True),\n    StructField('SupplierTransactionID', IntegerType(), True),\n    StructField('InvoiceID', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('SupplierInvoiceNumber', IntegerType(), True),\n    StructField('TotalExcludingTax', IntegerType(), True),\n    StructField('TaxAmount', IntegerType(), True),\n    StructField('TotalIncludingTax', DecimalType(), True),\n    StructField('OutstandingBalance', IntegerType(), True),\n    StructField('IsFinalized', StringType...: Compute snapshot for version: 0",
          "displayName": "toString at String.java:2994",
          "jobGroup": "21",
          "jobId": 152,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 8,
          "stageIds": [
           284
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:50.890GMT",
          "usageDescription": ""
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 9,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 21
      },
      "text/plain": [
       "StatementMeta(, 4259ac00-d937-4ed0-b39b-3a46ab7c1f3b, 21, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table_name = 'fact_transaction'\n",
    "dimension_city_schema = StructType([\n",
    "    StructField('TransactionKey', IntegerType(), True),\n",
    "    StructField('DateKey', StringType(), True), \n",
    "    StructField('CustomerKey', IntegerType(), True),\n",
    "    StructField('BillToCustomerKey', IntegerType(), True),\n",
    "    StructField('SupplierKey', IntegerType(), True),\n",
    "    StructField('TransactionTypekey', IntegerType(), True),\n",
    "    StructField('PaymentMethodKey', IntegerType(), True),\n",
    "    StructField('CustomerTransactionID', IntegerType(), True),\n",
    "    StructField('SupplierTransactionID', IntegerType(), True),\n",
    "    StructField('InvoiceID', IntegerType(), True),\n",
    "    StructField('PurchaseOrderID', IntegerType(), True),\n",
    "    StructField('SupplierInvoiceNumber', IntegerType(), True),\n",
    "    StructField('TotalExcludingTax', IntegerType(), True),\n",
    "    StructField('TaxAmount', IntegerType(), True),\n",
    "    StructField('TotalIncludingTax', DecimalType(), True),\n",
    "    StructField('OutstandingBalance', IntegerType(), True),\n",
    "    StructField('IsFinalized', StringType(), True),\n",
    "    StructField('LineageKey', IntegerType(), True)])\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\n",
    "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\"+table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f5896a-158a-4b95-aa67-acbed50d323a",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Fact-Stockholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "916c03ab-8d76-406d-b345-463fa9ee97c7",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-09-11T18:47:59.39825Z",
       "execution_start_time": "2023-09-11T18:47:55.7519814Z",
       "livy_statement_state": "available",
       "parent_msg_id": "e70aa17a-d25c-4527-99d1-945b16eb6930",
       "queued_time": "2023-09-11T18:46:21.657855Z",
       "session_id": "4259ac00-d937-4ed0-b39b-3a46ab7c1f3b",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [
         {
          "completionTime": "2023-09-11T18:47:58.484GMT",
          "dataRead": 4448,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 22:\ntable_name = 'fact_stockholding'\ndimension_city_schema = StructType([\n    StructField('StockHoldingKey', IntegerType(), True), \n    StructField('StockItemKey', IntegerType(), True),\n    StructField('QuantityOnHand', IntegerType(), True),\n    StructField('BinLocation', StringType(), True),\n    StructField('LastStocktakeQuantity', IntegerType(), True),\n    StructField('LastCostPrice', IntegerType(), True),\n    StructField('ReorderLevel', IntegerType(), True),\n    StructField('TargetStockLevel', IntegerType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 2",
          "displayName": "toString at String.java:2994",
          "jobGroup": "22",
          "jobId": 169,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 53,
          "numTasks": 54,
          "rowCount": 50,
          "stageIds": [
           317,
           315,
           316
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:58.462GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:58.449GMT",
          "dataRead": 5128,
          "dataWritten": 4448,
          "description": "Delta: Job group for statement 22:\ntable_name = 'fact_stockholding'\ndimension_city_schema = StructType([\n    StructField('StockHoldingKey', IntegerType(), True), \n    StructField('StockItemKey', IntegerType(), True),\n    StructField('QuantityOnHand', IntegerType(), True),\n    StructField('BinLocation', StringType(), True),\n    StructField('LastStocktakeQuantity', IntegerType(), True),\n    StructField('LastCostPrice', IntegerType(), True),\n    StructField('ReorderLevel', IntegerType(), True),\n    StructField('TargetStockLevel', IntegerType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 2",
          "displayName": "toString at String.java:2994",
          "jobGroup": "22",
          "jobId": 168,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 3,
          "numTasks": 53,
          "rowCount": 60,
          "stageIds": [
           314,
           313
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:58.060GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:57.970GMT",
          "dataRead": 5423,
          "dataWritten": 5128,
          "description": "Delta: Job group for statement 22:\ntable_name = 'fact_stockholding'\ndimension_city_schema = StructType([\n    StructField('StockHoldingKey', IntegerType(), True), \n    StructField('StockItemKey', IntegerType(), True),\n    StructField('QuantityOnHand', IntegerType(), True),\n    StructField('BinLocation', StringType(), True),\n    StructField('LastStocktakeQuantity', IntegerType(), True),\n    StructField('LastCostPrice', IntegerType(), True),\n    StructField('ReorderLevel', IntegerType(), True),\n    StructField('TargetStockLevel', IntegerType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 2",
          "displayName": "toString at String.java:2994",
          "jobGroup": "22",
          "jobId": 167,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 3,
          "numCompletedStages": 1,
          "numCompletedTasks": 3,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 3,
          "rowCount": 20,
          "stageIds": [
           312
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:57.922GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:57.580GMT",
          "dataRead": 2009,
          "dataWritten": 0,
          "description": "Job group for statement 22:\ntable_name = 'fact_stockholding'\ndimension_city_schema = StructType([\n    StructField('StockHoldingKey', IntegerType(), True), \n    StructField('StockItemKey', IntegerType(), True),\n    StructField('QuantityOnHand', IntegerType(), True),\n    StructField('BinLocation', StringType(), True),\n    StructField('LastStocktakeQuantity', IntegerType(), True),\n    StructField('LastCostPrice', IntegerType(), True),\n    StructField('ReorderLevel', IntegerType(), True),\n    StructField('TargetStockLevel', IntegerType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "22",
          "jobId": 166,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 2,
          "numTasks": 52,
          "rowCount": 3,
          "stageIds": [
           310,
           311
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:57.496GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:57.425GMT",
          "dataRead": 6423,
          "dataWritten": 9686,
          "description": "Job group for statement 22:\ntable_name = 'fact_stockholding'\ndimension_city_schema = StructType([\n    StructField('StockHoldingKey', IntegerType(), True), \n    StructField('StockItemKey', IntegerType(), True),\n    StructField('QuantityOnHand', IntegerType(), True),\n    StructField('BinLocation', StringType(), True),\n    StructField('LastStocktakeQuantity', IntegerType(), True),\n    StructField('LastCostPrice', IntegerType(), True),\n    StructField('ReorderLevel', IntegerType(), True),\n    StructField('TargetStockLevel', IntegerType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "22",
          "jobId": 165,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 2,
          "rowCount": 454,
          "stageIds": [
           308,
           309
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:57.193GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:57.159GMT",
          "dataRead": 8456,
          "dataWritten": 6423,
          "description": "Job group for statement 22:\ntable_name = 'fact_stockholding'\ndimension_city_schema = StructType([\n    StructField('StockHoldingKey', IntegerType(), True), \n    StructField('StockItemKey', IntegerType(), True),\n    StructField('QuantityOnHand', IntegerType(), True),\n    StructField('BinLocation', StringType(), True),\n    StructField('LastStocktakeQuantity', IntegerType(), True),\n    StructField('LastCostPrice', IntegerType(), True),\n    StructField('ReorderLevel', IntegerType(), True),\n    StructField('TargetStockLevel', IntegerType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "22",
          "jobId": 164,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 454,
          "stageIds": [
           307
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:57.104GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:56.905GMT",
          "dataRead": 4443,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 22:\ntable_name = 'fact_stockholding'\ndimension_city_schema = StructType([\n    StructField('StockHoldingKey', IntegerType(), True), \n    StructField('StockItemKey', IntegerType(), True),\n    StructField('QuantityOnHand', IntegerType(), True),\n    StructField('BinLocation', StringType(), True),\n    StructField('LastStocktakeQuantity', IntegerType(), True),\n    StructField('LastCostPrice', IntegerType(), True),\n    StructField('ReorderLevel', IntegerType(), True),\n    StructField('TargetStockLevel', IntegerType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "22",
          "jobId": 163,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 52,
          "numTasks": 53,
          "rowCount": 50,
          "stageIds": [
           305,
           306,
           304
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:56.878GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:56.864GMT",
          "dataRead": 3270,
          "dataWritten": 4443,
          "description": "Delta: Job group for statement 22:\ntable_name = 'fact_stockholding'\ndimension_city_schema = StructType([\n    StructField('StockHoldingKey', IntegerType(), True), \n    StructField('StockItemKey', IntegerType(), True),\n    StructField('QuantityOnHand', IntegerType(), True),\n    StructField('BinLocation', StringType(), True),\n    StructField('LastStocktakeQuantity', IntegerType(), True),\n    StructField('LastCostPrice', IntegerType(), True),\n    StructField('ReorderLevel', IntegerType(), True),\n    StructField('TargetStockLevel', IntegerType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "22",
          "jobId": 162,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 2,
          "numTasks": 52,
          "rowCount": 57,
          "stageIds": [
           302,
           303
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:56.426GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T18:47:56.300GMT",
          "dataRead": 3838,
          "dataWritten": 3270,
          "description": "Delta: Job group for statement 22:\ntable_name = 'fact_stockholding'\ndimension_city_schema = StructType([\n    StructField('StockHoldingKey', IntegerType(), True), \n    StructField('StockItemKey', IntegerType(), True),\n    StructField('QuantityOnHand', IntegerType(), True),\n    StructField('BinLocation', StringType(), True),\n    StructField('LastStocktakeQuantity', IntegerType(), True),\n    StructField('LastCostPrice', IntegerType(), True),\n    StructField('ReorderLevel', IntegerType(), True),\n    StructField('TargetStockLevel', IntegerType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1",
          "displayName": "toString at String.java:2994",
          "jobGroup": "22",
          "jobId": 161,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 2,
          "numCompletedStages": 1,
          "numCompletedTasks": 2,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 2,
          "rowCount": 14,
          "stageIds": [
           301
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T18:47:56.249GMT",
          "usageDescription": ""
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 9,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 22
      },
      "text/plain": [
       "StatementMeta(, 4259ac00-d937-4ed0-b39b-3a46ab7c1f3b, 22, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table_name = 'fact_stockholding'\n",
    "dimension_city_schema = StructType([\n",
    "    StructField('StockHoldingKey', IntegerType(), True), \n",
    "    StructField('StockItemKey', IntegerType(), True),\n",
    "    StructField('QuantityOnHand', IntegerType(), True),\n",
    "    StructField('BinLocation', StringType(), True),\n",
    "    StructField('LastStocktakeQuantity', IntegerType(), True),\n",
    "    StructField('LastCostPrice', IntegerType(), True),\n",
    "    StructField('ReorderLevel', IntegerType(), True),\n",
    "    StructField('TargetStockLevel', IntegerType(), True),\n",
    "    StructField('LineageKey', IntegerType(), True)])\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\n",
    "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\"+table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c7273d-cd07-4f28-bdba-cd775223557f",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Fact-Campaigndata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c8c5e93-6b88-4f03-bcb9-626037cfad8d",
   "metadata": {
    "advisor": {
     "adviceMetadata": "{\"artifactId\":\"7eb7090e-e87e-4f18-86b5-44a0a1d259ed\",\"activityId\":\"791d9c65-d23e-4b4f-9165-1d4b1e928963\",\"applicationId\":\"application_1694465880208_0001\",\"jobGroupId\":\"5\",\"advices\":{\"info\":1}}"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "parent_msg_id": "43f0f552-24cb-40c8-b645-e35aca24d19b",
       "queued_time": "2023-09-11T20:59:27.0113322Z",
       "session_id": null,
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table_name = 'fact_campaigndata'\n",
    "\n",
    "df = spark.read.format(\"csv\").option(\"header\",\"true\").load(Path_Fact+table_name)\n",
    "df = df.select(col(\"Region\"),col(\"Country\"),col(\"ProductCategory\"),col(\"Campaign_ID\"),col(\"Campaign_Name\"),\n",
    "col(\"Qualification\"),col(\"Qualification_Number\"),col(\"Response_Status\"),col(\"Responses\"),col(\"Cost\"),col(\"Revenue\"),col(\"ROI\"),col(\"Lead_Generation\"),col(\"Revenue_Target\"),col(\"Customer_Segment\"),\n",
    "col(\"Profit\"),col(\"Marketing_Cost\"),col(\"CampaignID\"))\n",
    "\n",
    "df = df.withColumn(\"ProductCategory\",col(\"ProductCategory\").cast(\"string\"))\n",
    "df = df.withColumn(\"CampaignID\",col(\"CampaignID\").cast(\"integer\")) \n",
    "df = df.withColumn(\"Campaign_ID\",col(\"Campaign_ID\").cast(\"integer\"))\n",
    "df = df.withColumn(\"ROI\",col(\"ROI\").cast(\"integer\")) \n",
    "df = df.withColumn(\"Revenue_Target\",col(\"Revenue_Target\").cast(\"Double\"))\n",
    "df = df.withColumn(\"Cost\",col(\"Cost\").cast(\"Double\"))\n",
    "df = df.withColumn(\"Responses\",col(\"Responses\").cast(\"Double\"))\n",
    "df = df.withColumn(\"Revenue\",col(\"Revenue\").cast(\"Double\"))\n",
    "df = df.withColumn(\"Profit\",col(\"Profit\").cast(\"Double\"))\n",
    "df = df.withColumn(\"Marketing_Cost\",col(\"Marketing_Cost\").cast(\"Double\"))\n",
    "\n",
    "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\"+table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586cf245",
   "metadata": {},
   "source": [
    "## Fact-StoreSalesData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad468db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "table_name = 'fact_store_sales_data'\n",
    "Path_Fact='abfss://#SALES_WORKSPACE_NAME#@onelake.dfs.fabric.microsoft.com/#LAKEHOUSE_BRONZE#.Lakehouse/Files/FactData/'\n",
    "\n",
    "# Read the CSV file into a Spark DataFrame\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(Path_Fact + table_name)\n",
    "\n",
    "# Cast the columns to the appropriate data types\n",
    "df = df.withColumn(\"store\", col(\"store\").cast(\"integer\"))\n",
    "df = df.withColumn(\"item\", col(\"item\").cast(\"integer\"))\n",
    "df = df.withColumn(\"sales\", col(\"sales\").cast(\"integer\"))\n",
    "\n",
    "# Save the DataFrame as a Delta table\n",
    "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\" + table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26418a2d",
   "metadata": {},
   "source": [
    "## Fact-Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805b1f7f-c70c-4f7c-ab05-4b7638ffd4f3",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-09-11T19:06:41.0859598Z",
       "execution_start_time": "2023-09-11T19:06:30.9508492Z",
       "livy_statement_state": "available",
       "parent_msg_id": "c13e4c92-e2a1-4b16-99f4-4ac4872a6775",
       "queued_time": "2023-09-11T19:06:30.5417676Z",
       "session_id": "4259ac00-d937-4ed0-b39b-3a46ab7c1f3b",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [
         {
          "completionTime": "2023-09-11T19:06:38.865GMT",
          "dataRead": 4663,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 37:\ntable_name='fact_sales'\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(Path_LitwareData+'/*/*/*.txt')\ndf = df.select(col(\"CustomerId\"),col(\"ProductId\"),col(\"Quantity\"),col(\"Price\"),col(\"TotalAmount\"),col(\"TransactionDate\"),col(\"ProfitAmount\"),col(\"StoreId\"))\n\ndf = df.withColumn(\"ProductId\",col(\"ProductId\").cast(\"integer\"))\ndf = df.withColumn(\"CustomerId\",col(\"CustomerId\").cast(\"integer\"))\ndf = df.withColumn(\"Quantity\",col(\"Quantity\").cast(\"integer\"))\ndf = df.withColumn(\"Price\",col(\"Price\").cast(\"integer\"))\ndf = df.withColumn(\"TotalAmount\",col(\"TotalAmount\").cast(\"Double\"))\ndf = df.withColumn(\"ProfitAmount\",col(\"ProfitAmount\").cast(\"Double\"))\ndf = df.withColumn(\"StoreId\",col(\"StoreId\").cast(\"integer\"))\ndf = df.withColumn(\"TransactionDate\",to_date(unix_timestamp(col('TransactionDate'), 'MM-dd-yyyy').cast(\"timestamp\")))\n\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 5",
          "displayName": "toString at String.java:2994",
          "jobGroup": "37",
          "jobId": 195,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 56,
          "numTasks": 57,
          "rowCount": 50,
          "stageIds": [
           357,
           358,
           359
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T19:06:38.833GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T19:06:38.812GMT",
          "dataRead": 428198,
          "dataWritten": 4663,
          "description": "Delta: Job group for statement 37:\ntable_name='fact_sales'\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(Path_LitwareData+'/*/*/*.txt')\ndf = df.select(col(\"CustomerId\"),col(\"ProductId\"),col(\"Quantity\"),col(\"Price\"),col(\"TotalAmount\"),col(\"TransactionDate\"),col(\"ProfitAmount\"),col(\"StoreId\"))\n\ndf = df.withColumn(\"ProductId\",col(\"ProductId\").cast(\"integer\"))\ndf = df.withColumn(\"CustomerId\",col(\"CustomerId\").cast(\"integer\"))\ndf = df.withColumn(\"Quantity\",col(\"Quantity\").cast(\"integer\"))\ndf = df.withColumn(\"Price\",col(\"Price\").cast(\"integer\"))\ndf = df.withColumn(\"TotalAmount\",col(\"TotalAmount\").cast(\"Double\"))\ndf = df.withColumn(\"ProfitAmount\",col(\"ProfitAmount\").cast(\"Double\"))\ndf = df.withColumn(\"StoreId\",col(\"StoreId\").cast(\"integer\"))\ndf = df.withColumn(\"TransactionDate\",to_date(unix_timestamp(col('TransactionDate'), 'MM-dd-yyyy').cast(\"timestamp\")))\n\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 5",
          "displayName": "toString at String.java:2994",
          "jobGroup": "37",
          "jobId": 194,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 6,
          "numTasks": 56,
          "rowCount": 3891,
          "stageIds": [
           356,
           355
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T19:06:38.231GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T19:06:38.115GMT",
          "dataRead": 1883321,
          "dataWritten": 428198,
          "description": "Delta: Job group for statement 37:\ntable_name='fact_sales'\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(Path_LitwareData+'/*/*/*.txt')\ndf = df.select(col(\"CustomerId\"),col(\"ProductId\"),col(\"Quantity\"),col(\"Price\"),col(\"TotalAmount\"),col(\"TransactionDate\"),col(\"ProfitAmount\"),col(\"StoreId\"))\n\ndf = df.withColumn(\"ProductId\",col(\"ProductId\").cast(\"integer\"))\ndf = df.withColumn(\"CustomerId\",col(\"CustomerId\").cast(\"integer\"))\ndf = df.withColumn(\"Quantity\",col(\"Quantity\").cast(\"integer\"))\ndf = df.withColumn(\"Price\",col(\"Price\").cast(\"integer\"))\ndf = df.withColumn(\"TotalAmount\",col(\"TotalAmount\").cast(\"Double\"))\ndf = df.withColumn(\"ProfitAmount\",col(\"ProfitAmount\").cast(\"Double\"))\ndf = df.withColumn(\"StoreId\",col(\"StoreId\").cast(\"integer\"))\ndf = df.withColumn(\"TransactionDate\",to_date(unix_timestamp(col('TransactionDate'), 'MM-dd-yyyy').cast(\"timestamp\")))\n\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 5",
          "displayName": "toString at String.java:2994",
          "jobGroup": "37",
          "jobId": 193,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 6,
          "numCompletedStages": 1,
          "numCompletedTasks": 6,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 6,
          "rowCount": 7682,
          "stageIds": [
           354
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T19:06:37.969GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T19:06:37.513GMT",
          "dataRead": 186764,
          "dataWritten": 0,
          "description": "Job group for statement 37:\ntable_name='fact_sales'\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(Path_LitwareData+'/*/*/*.txt')\ndf = df.select(col(\"CustomerId\"),col(\"ProductId\"),col(\"Quantity\"),col(\"Price\"),col(\"TotalAmount\"),col(\"TransactionDate\"),col(\"ProfitAmount\"),col(\"StoreId\"))\n\ndf = df.withColumn(\"ProductId\",col(\"ProductId\").cast(\"integer\"))\ndf = df.withColumn(\"CustomerId\",col(\"CustomerId\").cast(\"integer\"))\ndf = df.withColumn(\"Quantity\",col(\"Quantity\").cast(\"integer\"))\ndf = df.withColumn(\"Price\",col(\"Price\").cast(\"integer\"))\ndf = df.withColumn(\"TotalAmount\",col(\"TotalAmount\").cast(\"Double\"))\ndf = df.withColumn(\"ProfitAmount\",col(\"ProfitAmount\").cast(\"Double\"))\ndf = df.withColumn(\"StoreId\",col(\"StoreId\").cast(\"integer\"))\ndf = df.withColumn(\"TransactionDate\",to_date(unix_timestamp(col('TransactionDate'), 'MM-dd-yyyy').cast(\"timestamp\")))\n\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "37",
          "jobId": 192,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 5,
          "numTasks": 55,
          "rowCount": 1918,
          "stageIds": [
           353,
           352
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T19:06:37.398GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T19:06:37.337GMT",
          "dataRead": 50088514,
          "dataWritten": 12832141,
          "description": "Job group for statement 37:\ntable_name='fact_sales'\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(Path_LitwareData+'/*/*/*.txt')\ndf = df.select(col(\"CustomerId\"),col(\"ProductId\"),col(\"Quantity\"),col(\"Price\"),col(\"TotalAmount\"),col(\"TransactionDate\"),col(\"ProfitAmount\"),col(\"StoreId\"))\n\ndf = df.withColumn(\"ProductId\",col(\"ProductId\").cast(\"integer\"))\ndf = df.withColumn(\"CustomerId\",col(\"CustomerId\").cast(\"integer\"))\ndf = df.withColumn(\"Quantity\",col(\"Quantity\").cast(\"integer\"))\ndf = df.withColumn(\"Price\",col(\"Price\").cast(\"integer\"))\ndf = df.withColumn(\"TotalAmount\",col(\"TotalAmount\").cast(\"Double\"))\ndf = df.withColumn(\"ProfitAmount\",col(\"ProfitAmount\").cast(\"Double\"))\ndf = df.withColumn(\"StoreId\",col(\"StoreId\").cast(\"integer\"))\ndf = df.withColumn(\"TransactionDate\",to_date(unix_timestamp(col('TransactionDate'), 'MM-dd-yyyy').cast(\"timestamp\")))\n\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "37",
          "jobId": 191,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 8,
          "numTasks": 9,
          "rowCount": 4000006,
          "stageIds": [
           350,
           351
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T19:06:35.344GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T19:06:35.309GMT",
          "dataRead": 170830001,
          "dataWritten": 50088514,
          "description": "Job group for statement 37:\ntable_name='fact_sales'\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(Path_LitwareData+'/*/*/*.txt')\ndf = df.select(col(\"CustomerId\"),col(\"ProductId\"),col(\"Quantity\"),col(\"Price\"),col(\"TotalAmount\"),col(\"TransactionDate\"),col(\"ProfitAmount\"),col(\"StoreId\"))\n\ndf = df.withColumn(\"ProductId\",col(\"ProductId\").cast(\"integer\"))\ndf = df.withColumn(\"CustomerId\",col(\"CustomerId\").cast(\"integer\"))\ndf = df.withColumn(\"Quantity\",col(\"Quantity\").cast(\"integer\"))\ndf = df.withColumn(\"Price\",col(\"Price\").cast(\"integer\"))\ndf = df.withColumn(\"TotalAmount\",col(\"TotalAmount\").cast(\"Double\"))\ndf = df.withColumn(\"ProfitAmount\",col(\"ProfitAmount\").cast(\"Double\"))\ndf = df.withColumn(\"StoreId\",col(\"StoreId\").cast(\"integer\"))\ndf = df.withColumn(\"TransactionDate\",to_date(unix_timestamp(col('TransactionDate'), 'MM-dd-yyyy').cast(\"timestamp\")))\n\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "jobGroup": "37",
          "jobId": 190,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 8,
          "numCompletedStages": 1,
          "numCompletedTasks": 8,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 8,
          "rowCount": 4000006,
          "stageIds": [
           349
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T19:06:32.411GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-09-11T19:06:31.707GMT",
          "dataRead": 65536,
          "dataWritten": 0,
          "description": "Job group for statement 37:\ntable_name='fact_sales'\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(Path_LitwareData+'/*/*/*.txt')\ndf = df.select(col(\"CustomerId\"),col(\"ProductId\"),col(\"Quantity\"),col(\"Price\"),col(\"TotalAmount\"),col(\"TransactionDate\"),col(\"ProfitAmount\"),col(\"StoreId\"))\n\ndf = df.withColumn(\"ProductId\",col(\"ProductId\").cast(\"integer\"))\ndf = df.withColumn(\"CustomerId\",col(\"CustomerId\").cast(\"integer\"))\ndf = df.withColumn(\"Quantity\",col(\"Quantity\").cast(\"integer\"))\ndf = df.withColumn(\"Price\",col(\"Price\").cast(\"integer\"))\ndf = df.withColumn(\"TotalAmount\",col(\"TotalAmount\").cast(\"Double\"))\ndf = df.withColumn(\"ProfitAmount\",col(\"ProfitAmount\").cast(\"Double\"))\ndf = df.withColumn(\"StoreId\",col(\"StoreId\").cast(\"integer\"))\ndf = df.withColumn(\"TransactionDate\",to_date(unix_timestamp(col('TransactionDate'), 'MM-dd-yyyy').cast(\"timestamp\")))\n\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)",
          "displayName": "load at <unknown>:0",
          "jobGroup": "37",
          "jobId": 189,
          "killedTasksSummary": {},
          "name": "load at <unknown>:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 1,
          "stageIds": [
           348
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-09-11T19:06:31.558GMT",
          "usageDescription": ""
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 7,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 37
      },
      "text/plain": [
       "StatementMeta(, 4259ac00-d937-4ed0-b39b-3a46ab7c1f3b, 37, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date, unix_timestamp\n",
    "\n",
    "table_name = 'fact_sales'\n",
    "Path_LitwareData='abfss://#SALES_WORKSPACE_NAME#@onelake.dfs.fabric.microsoft.com/#LAKEHOUSE_BRONZE#.Lakehouse/Files/sales-transaction-litware'\n",
    "\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(Path_LitwareData + '/*/*/*.txt')\n",
    "\n",
    "# Select relevant columns\n",
    "df = df.select(col(\"CustomerId\"), col(\"ProductId\"), col(\"Quantity\"), col(\"Price\"), col(\"TotalAmount\"), col(\"TransactionDate\"), col(\"ProfitAmount\"), col(\"StoreId\"))\n",
    "\n",
    "# Cast columns to appropriate data types\n",
    "df = df.withColumn(\"ProductId\", col(\"ProductId\").cast(\"integer\"))\n",
    "df = df.withColumn(\"CustomerId\", col(\"CustomerId\").cast(\"integer\"))\n",
    "df = df.withColumn(\"Quantity\", col(\"Quantity\").cast(\"integer\"))\n",
    "df = df.withColumn(\"Price\", col(\"Price\").cast(\"integer\"))\n",
    "df = df.withColumn(\"TotalAmount\", col(\"TotalAmount\").cast(\"double\"))\n",
    "df = df.withColumn(\"ProfitAmount\", col(\"ProfitAmount\").cast(\"double\"))\n",
    "df = df.withColumn(\"StoreId\", col(\"StoreId\").cast(\"integer\"))\n",
    "df = df.withColumn(\"TransactionDate\", to_date(unix_timestamp(col('TransactionDate'), 'MM-dd-yyyy').cast(\"timestamp\")))\n",
    "\n",
    "# Save DataFrame as a table in Delta format\n",
    "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\" + table_name)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "host": {
    "synapse_widget": {
     "state": {},
     "token": "ade1524a-8ed7-4bad-8cd1-4ff6a91fe735"
    }
   },
   "language": "python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "notebook_environment": {},
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "save_output": true,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {},
    "enableDebugMode": false
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "trident": {
   "lakehouse": {
    "default_lakehouse": "65cb467b-e047-4716-9d4b-6bc2d1394a72",
    "default_lakehouse_name": "lakehouseSilver",
    "default_lakehouse_workspace_id": "a5271330-478b-4f25-b793-db6ec1971842",
    "known_lakehouses": [
     {
      "id": "65cb467b-e047-4716-9d4b-6bc2d1394a72"
     }
    ]
   }
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
