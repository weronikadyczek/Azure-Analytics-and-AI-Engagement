{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9106ef3-b532-41f2-a123-8ab55afc464d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Send Sensor Telemtry to RTI\n",
    "Stream data from the SensorData.csv file to RTI for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9a0083-09c6-42cc-98d7-f99fab93d619",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Install the required package\n",
    "%pip install azure-eventhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "071e3896-3701-4511-ac94-459e00d2e179",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-07-01T21:10:17.0602194Z",
       "execution_start_time": "2025-07-01T21:10:16.7621371Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "39a3f1f6-9e4f-4c83-a3a1-fc18373548e4",
       "queued_time": "2025-07-01T21:09:45.2418557Z",
       "session_id": "be711a4a-5123-4f35-a65e-7aadddafc4b1",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 10,
       "statement_ids": [
        10
       ]
      },
      "text/plain": [
       "StatementMeta(, be711a4a-5123-4f35-a65e-7aadddafc4b1, 10, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import required libraries\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from azure.eventhub import EventHubConsumerClient, EventHubProducerClient,EventData\n",
    "import csv\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d3e24a",
   "metadata": {},
   "source": [
    "### Test the Data Connections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380aab2e-3940-43f7-9f71-876f0ccf8182",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-07-01T21:10:22.0180774Z",
       "execution_start_time": "2025-07-01T21:10:17.3460151Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "1ca1c780-1253-4e78-8250-0aa8d7e76555",
       "queued_time": "2025-07-01T21:09:45.2457518Z",
       "session_id": "be711a4a-5123-4f35-a65e-7aadddafc4b1",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 12,
       "statement_ids": [
        12
       ]
      },
      "text/plain": [
       "StatementMeta(, be711a4a-5123-4f35-a65e-7aadddafc4b1, 12, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+----+--------+---------+\n",
      "|          timestamp|sensorValue|unit|sensorId|sessionID|\n",
      "+-------------------+-----------+----+--------+---------+\n",
      "|2023-01-01 00:00:00|       50.5| kPa| SEN0003| S0000150|\n",
      "|2023-01-01 00:00:01|      85.32| kPa| SEN0027| S0000150|\n",
      "|2023-01-01 00:00:02|      78.35| kPa| SEN0051| S0000150|\n",
      "|2023-01-01 00:00:03|      37.18| kPa| SEN0075| S0000150|\n",
      "|2023-01-01 00:00:04|      61.45| kPa| SEN0099| S0000150|\n",
      "|2023-01-01 00:00:05|      61.16| kPa| SEN0123| S0000150|\n",
      "|2023-01-01 00:00:06|      65.89| kPa| SEN0003| S0000150|\n",
      "|2023-01-01 00:00:07|      88.39| kPa| SEN0027| S0000150|\n",
      "|2023-01-01 00:00:08|      71.91| kPa| SEN0051| S0000150|\n",
      "|2023-01-01 00:00:09|      54.37| kPa| SEN0075| S0000150|\n",
      "|2023-01-01 00:00:10|      66.26| kPa| SEN0099| S0000150|\n",
      "|2023-01-01 00:00:11|       45.3| kPa| SEN0123| S0000150|\n",
      "|2023-01-01 00:00:12|      51.54| kPa| SEN0003| S0000150|\n",
      "|2023-01-01 00:00:13|       33.3| kPa| SEN0027| S0000150|\n",
      "|2023-01-01 00:00:14|      48.41| kPa| SEN0051| S0000150|\n",
      "|2023-01-01 00:00:15|      75.65| kPa| SEN0075| S0000150|\n",
      "|2023-01-01 00:00:16|      30.08| kPa| SEN0099| S0000150|\n",
      "|2023-01-01 00:00:17|      35.05| kPa| SEN0123| S0000150|\n",
      "|2023-01-01 00:00:18|      77.97| kPa| SEN0003| S0000150|\n",
      "|2023-01-01 00:00:19|      74.96| kPa| SEN0027| S0000150|\n",
      "+-------------------+-----------+----+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Cell to ensure the file is accessible and has data - Optional to run.\n",
    "\n",
    "# Define the path to the CSV file in the Lakehouse \n",
    "# UPDATE WITH YOUR LAKEHOUSE\n",
    "file_path = \"abfss://<###YOUR WORKSPACE###>@onelake.dfs.fabric.microsoft.com/ZavaLakehouse.Lakehouse/Files/RawData/SensorData/SensorData.csv\"\n",
    "\n",
    "# Load the CSV into a DataFrame\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(file_path)\n",
    "\n",
    "# Show the first few rows\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c99f95-1fee-46be-ad23-ee2039f2a3d6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Start the data generation below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f39a514-514c-43ee-8f0d-c8c410b6ff33",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1 Setup Connection Info\n",
    "connection_string = \"Endpoint=sb://<###YOUR RTI ENDPOINT###>.servicebus.windows.net/;\";\n",
    "producer = EventHubProducerClient.from_connection_string(connection_string)\n",
    "\n",
    "# Step 2: Read CSV file from Lakehouse Files\n",
    "\n",
    "file_path = \"abfss://<###YOUR WORKSPACE###>@onelake.dfs.fabric.microsoft.com/ZavaLakehouse.Lakehouse/Files/RawData/SensorData/SensorDataShoe.csv\"\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(file_path)\n",
    "\n",
    "# Step 2: Convert DataFrame to JSON rows\n",
    "json_rows = df.toJSON().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea2234e-a6f8-4ae0-87fb-2f056038fd8d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Step 4: Send 10 rows every 1 second and loop back to the beginning\n",
    "with producer:\n",
    "   while True:\n",
    "     for i in range(0, len(json_rows), 10):\n",
    "          batch = json_rows[i:i+10]\n",
    "          event_data_batch = [EventData(row) for row in batch]\n",
    "          producer.send_batch(event_data_batch)\n",
    "          # print(f\"Sent batch {i//10 + 1}\")\n",
    "          time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
