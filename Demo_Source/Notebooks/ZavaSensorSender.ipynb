{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9106ef3-b532-41f2-a123-8ab55afc464d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Send Sensor Telemtry to RTI\n",
    "Stream data from the SensorData.csv file to RTI for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9a0083-09c6-42cc-98d7-f99fab93d619",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Install the required package\n",
    "%pip install azure-eventhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071e3896-3701-4511-ac94-459e00d2e179",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from azure.eventhub import EventHubConsumerClient, EventHubProducerClient,EventData\n",
    "import csv\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8877d25",
   "metadata": {},
   "source": [
    "### Test the Data Connections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380aab2e-3940-43f7-9f71-876f0ccf8182",
   "metadata": {
    "advisor": {
     "adviceMetadata": "{\"artifactId\":\"d0428a4c-9a25-4e10-9beb-0a6552268926\",\"activityId\":\"4238ce90-50ae-4be3-9f93-aa71ccc999fb\",\"applicationId\":\"application_1751435920691_0001\",\"jobGroupId\":\"12\",\"advices\":{\"error\":1}}"
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell to ensure the file is accessible and has data - Optional to run.\n",
    "\n",
    "# Define the path to the CSV file in the Lakehouse \n",
    "# UPDATE WITH YOUR LAKEHOUSE\n",
    "file_path = \"abfss://<###YOUR WORKSPACE###>@onelake.dfs.fabric.microsoft.com/ZavaLakehouse.Lakehouse/Files/RawData/SensorData/SensorData.csv\"\n",
    "\n",
    "# Load the CSV into a DataFrame\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(file_path)\n",
    "\n",
    "# Show the first few rows\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c99f95-1fee-46be-ad23-ee2039f2a3d6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Start the data generation below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f39a514-514c-43ee-8f0d-c8c410b6ff33",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1 Setup Connection Info\n",
    "connection_string = \"Endpoint=<###YOUR RTI ENDPOINT###>\";\n",
    "producer = EventHubProducerClient.from_connection_string(connection_string)\n",
    "\n",
    "# Step 2: Read CSV file from Lakehouse Files\n",
    "\n",
    "file_path = \"abfss://<###YOUR WORKSPACE###>@onelake.dfs.fabric.microsoft.com/ZavaLakehouse.Lakehouse/Files/RawData/SensorData/SensorData.csv\"\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(file_path)\n",
    "\n",
    "# Step 2: Convert DataFrame to JSON rows\n",
    "json_rows = df.toJSON().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea2234e-a6f8-4ae0-87fb-2f056038fd8d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Step 4: Send 10 rows every 1 second and loop back to the beginning, manually stop this cell when you are done sending data\n",
    "with producer:\n",
    "   while True:\n",
    "     for i in range(0, len(json_rows), 10):\n",
    "          batch = json_rows[i:i+10]\n",
    "          event_data_batch = [EventData(row) for row in batch]\n",
    "          producer.send_batch(event_data_batch)\n",
    "          # print(f\"Sent batch {i//10 + 1}\")\n",
    "          time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
